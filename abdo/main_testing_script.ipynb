{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsforecast import StatsForecast\n",
    "from prophet import Prophet\n",
    "\n",
    "import re\n",
    "from statsforecast.models import (\n",
    "    # HoltWinters,\n",
    "    # CrostonClassic as Croston, \n",
    "    # HistoricAverage,\n",
    "    DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive,\n",
    "    # AutoARIMA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_table = extract_dummy_data(\"dummy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pointID</th>\n",
       "      <th>unit</th>\n",
       "      <th>dqType</th>\n",
       "      <th>dqStart</th>\n",
       "      <th>dqDuration</th>\n",
       "      <th>pointInterval</th>\n",
       "      <th>features</th>\n",
       "      <th>his</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "      <td>°C</td>\n",
       "      <td>Nulls</td>\n",
       "      <td>2023-03-12 01:05:00+04:00</td>\n",
       "      <td>1 days 11:10:00</td>\n",
       "      <td>0 days 00:05:00</td>\n",
       "      <td>[p:dmc_All:r:2de337c0-72b69972]</td>\n",
       "      <td>ts  \\\n",
       "0    2023-02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "      <td>°C</td>\n",
       "      <td>Nulls</td>\n",
       "      <td>2023-03-19 01:10:00+04:00</td>\n",
       "      <td>0 days 23:30:00</td>\n",
       "      <td>0 days 00:05:00</td>\n",
       "      <td>[p:dmc_All:r:2de337c0-72b69972]</td>\n",
       "      <td>ts  \\\n",
       "0    2023-03...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pointID unit dqType  \\\n",
       "0  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...   °C  Nulls   \n",
       "1  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...   °C  Nulls   \n",
       "\n",
       "                    dqStart      dqDuration   pointInterval  \\\n",
       "0 2023-03-12 01:05:00+04:00 1 days 11:10:00 0 days 00:05:00   \n",
       "1 2023-03-19 01:10:00+04:00 0 days 23:30:00 0 days 00:05:00   \n",
       "\n",
       "                          features  \\\n",
       "0  [p:dmc_All:r:2de337c0-72b69972]   \n",
       "1  [p:dmc_All:r:2de337c0-72b69972]   \n",
       "\n",
       "                                                 his  \n",
       "0                              ts  \\\n",
       "0    2023-02...  \n",
       "1                              ts  \\\n",
       "0    2023-03...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "# TESTING \n",
    "#---------\n",
    "\n",
    "# Testing variables\n",
    "row = master_table.iloc[1]\n",
    "df = row[\"his\"]\n",
    "df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "length_of_missing_data = row[\"dqDuration\"]\n",
    "data_logging_interval = row[\"pointInterval\"]\n",
    "dqStart\t= row['dqStart']\n",
    "dqDuration = row['dqDuration']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_naive(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        SeasonalNaive(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"SeasonalNaive\":\"seasonalNaive\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        DOT(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "\n",
    "    # renaming columns\n",
    "    df.columns = ['ds', 'temp']\n",
    "\n",
    "    # Remove ' Dubai' from the datetime strings\n",
    "    df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "    # Try converting the 'ds' column to datetime format with error handling\n",
    "    try:\n",
    "        df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing datetime: {e}\")\n",
    "        print(\"Some datetime strings could not be parsed. Check your data.\")\n",
    "        problematic_rows = df[pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\", errors='coerce').isna()]\n",
    "        print(\"Problematic rows:\")\n",
    "        print(problematic_rows)\n",
    "        return None\n",
    "\n",
    "    # Drop rows where datetime parsing failed\n",
    "    df = df.dropna(subset=['ds'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    # Separate data for temperature\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                         interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                         changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(df_temp)\n",
    "\n",
    "    # Calculate number of predictions\n",
    "    samples = int(length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Specify the start date for prediction\n",
    "    dq_start = pd.Timestamp(dqStart, tz='Asia/Dubai').tz_localize(None)\n",
    "\n",
    "    # Create future DataFrame starting from dq_start\n",
    "    future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "    # Adjust 'ds' column to start from dq_start\n",
    "    future_temp['ds'] = dq_start + pd.to_timedelta(range(len(future_temp)), unit='m')\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    future_temp['ds'] = future_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "    # Ensure 'ds' column in forecast_temp is timezone-naive\n",
    "    forecast_temp['ds'] = forecast_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Convert dq_start to timezone-naive\n",
    "    dq_start = dq_start.tz_localize(None)\n",
    "\n",
    "    # Filter predictions to start from dq_start\n",
    "    predictions = forecast_temp[forecast_temp['ds'] >= dq_start][['ds', 'yhat']]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(python_master_table):\n",
    "    \"\"\"\n",
    "    Function to run all models, and return the one with lowest RMSE.\n",
    "    Models running through the ensemble model will have input DataFrame (AKA the \"his\" column on master_table) \n",
    "    with timestamp as index, target variable as first column, feature variables as the rest of the columns.\n",
    "\n",
    "    Make sure the output predictions of all models are INCLUSIVE of both the \"start ts\" and \"end ts\" (AKA\n",
    "    last ts with real data before gap, and first ts with real data after gap) \n",
    "\n",
    "    Make sure to follow camelCase for DataFrame column naming for compatibility with SS\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary to save predictions for each point\n",
    "    scores_df_dict = {\n",
    "    \"pointID\": [],\n",
    "    \"predictions\": [],\n",
    "    \"rmse\": [],\n",
    "    \"modelName\": []\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "    for i, row in python_master_table.iterrows():\n",
    "\n",
    "        #-----------------\n",
    "        # INPUTS TO MODELS\n",
    "        #-----------------\n",
    "\n",
    "        pointID = row[\"pointID\"]\n",
    "        df = row[\"his\"]#.to_dataframe()                           #### IMPORTANT : UNCOMMENT THIS ON SS\n",
    "        df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "        length_of_missing_data = row[\"dqDuration\"]\n",
    "        data_logging_interval = row[\"pointInterval\"]\n",
    "        dqStart = row[\"dqStart\"]\n",
    "\n",
    "        #----------------------------\n",
    "        # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE \n",
    "        #----------------------------\n",
    "\n",
    "\n",
    "        # UNIVARIATE Models\n",
    "        dq_models = {\n",
    "            \"Seasonal Naive\" : seasonal_naive,\n",
    "            \"Dynamic Optimized Theta\": dynamic_optimized_theta\n",
    "        }\n",
    "\n",
    "        # MULTIVARIATE Models\n",
    "        # dq_models_mv = .....\n",
    "\n",
    "        # if len(df.columns) > 1 ===> use multivariate models dictionary \n",
    "\n",
    "        for model_name, model in dq_models.items():\n",
    "            \n",
    "            #------------------------\n",
    "            # ** Calculating RMSE **\n",
    "            #------------------------\n",
    "\n",
    "            # number of predictions needed\n",
    "            horizon = int(length_of_missing_data/data_logging_interval) +1 # why +1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions INCLUSIVE of BOTH start and end ts\n",
    "\n",
    "            # training set size (relative to the horizon/prediction size)\n",
    "            training_set_size = horizon * 10\n",
    "\n",
    "            # training / testing set to evaluate the model (relative to horizon of prediction)\n",
    "            train_data = df.iloc[-1*int(training_set_size):-1*int(horizon)]\n",
    "            test_data = df.iloc[-1*int(horizon):]\n",
    "\n",
    "            # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "            predictions_for_rmse = model(train_data, length_of_missing_data, data_logging_interval, dqStart)\n",
    "            rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "\n",
    "            #------------------\n",
    "            # ** Predictions **\n",
    "            #------------------\n",
    "\n",
    "            # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "            predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "            # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "            start = row['dqStart']\n",
    "            duration = row['dqDuration']\n",
    "            interval = row['pointInterval']\n",
    "            timestamps = pd.date_range(start=start, end=start+duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "            predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "            # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "            predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "            # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "            predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "            # append data to the scores DF\n",
    "            row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                            \"rmse\": rmse_score, \"modelName\": model_name, \n",
    "                            \"identifier\": \n",
    "                                str(row[\"pointID\"])\n",
    "                                +str(row[\"dqStart\"])\n",
    "                                +str(row[\"dqDuration\"])\n",
    "                                +str(row[\"dqType\"])}\n",
    "            \n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "            # return predictions with least RMSE for each point/dq issue\n",
    "            idx = scores_df.groupby('identifier')['rmse'].idxmin()\n",
    "            scores_df = scores_df.loc[idx].reset_index(drop=True)\n",
    "            \n",
    "    return scores_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "a = ensemble_model(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DMC Building 1 Data Quality Tests Dup of AHU_04_B1 Return Air Temp</th>\n",
       "      <th>DMC Building 1 Data Quality Tests New-Point</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-02 09:10:00+04:00</th>\n",
       "      <td>23.060774</td>\n",
       "      <td>38.153641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02 09:15:00+04:00</th>\n",
       "      <td>23.210907</td>\n",
       "      <td>38.153641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02 09:20:00+04:00</th>\n",
       "      <td>23.210907</td>\n",
       "      <td>38.153641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02 09:25:00+04:00</th>\n",
       "      <td>23.210907</td>\n",
       "      <td>38.153641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-02 09:30:00+04:00</th>\n",
       "      <td>23.362026</td>\n",
       "      <td>38.153641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-19 00:45:00+04:00</th>\n",
       "      <td>22.633480</td>\n",
       "      <td>29.431870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-19 00:50:00+04:00</th>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.548756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-19 00:55:00+04:00</th>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-19 01:00:00+04:00</th>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-19 01:05:00+04:00</th>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4370 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           DMC Building 1 Data Quality Tests Dup of AHU_04_B1 Return Air Temp  \\\n",
       "ts                                                                                              \n",
       "2023-03-02 09:10:00+04:00                                          23.060774                    \n",
       "2023-03-02 09:15:00+04:00                                          23.210907                    \n",
       "2023-03-02 09:20:00+04:00                                          23.210907                    \n",
       "2023-03-02 09:25:00+04:00                                          23.210907                    \n",
       "2023-03-02 09:30:00+04:00                                          23.362026                    \n",
       "...                                                                      ...                    \n",
       "2023-03-19 00:45:00+04:00                                          22.633480                    \n",
       "2023-03-19 00:50:00+04:00                                          22.633480                    \n",
       "2023-03-19 00:55:00+04:00                                          22.633480                    \n",
       "2023-03-19 01:00:00+04:00                                          22.633480                    \n",
       "2023-03-19 01:05:00+04:00                                          22.633480                    \n",
       "\n",
       "                           DMC Building 1 Data Quality Tests New-Point  \n",
       "ts                                                                      \n",
       "2023-03-02 09:10:00+04:00                                    38.153641  \n",
       "2023-03-02 09:15:00+04:00                                    38.153641  \n",
       "2023-03-02 09:20:00+04:00                                    38.153641  \n",
       "2023-03-02 09:25:00+04:00                                    38.153641  \n",
       "2023-03-02 09:30:00+04:00                                    38.153641  \n",
       "...                                                                ...  \n",
       "2023-03-19 00:45:00+04:00                                    29.431870  \n",
       "2023-03-19 00:50:00+04:00                                    30.548756  \n",
       "2023-03-19 00:55:00+04:00                                    30.514156  \n",
       "2023-03-19 01:00:00+04:00                                    30.514156  \n",
       "2023-03-19 01:05:00+04:00                                    30.514156  \n",
       "\n",
       "[4370 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_regression(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    dqStart: start of the predictions\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Splitting variables\n",
    "    y = df[df.columns[0]]  # independent variable\n",
    "    X = df[[df.columns[1]]]  # dependent variable\n",
    "\n",
    "    # Filter data for training and testing\n",
    "    X_train = X[X.index < dqStart]\n",
    "    y_train = y[X.index < dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    #y_test = y[X.index >= dqStart]\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree = 4)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # Train polynomial regression model on the whole dataset\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    # Create a new DataFrame with the timestamp as index and y_pred as values\n",
    "    pred_df = pd.DataFrame(data=y_pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "    return pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
