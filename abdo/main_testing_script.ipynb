{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "# imports for DOT and seasonalNaive models\n",
    "import re\n",
    "from statsforecast.models import (\n",
    "    # HoltWinters,\n",
    "    # CrostonClassic as Croston, \n",
    "    # HistoricAverage,\n",
    "    DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive,\n",
    "    # AutoARIMA\n",
    ")\n",
    "from statsforecast import StatsForecast\n",
    "# imports for polynomialRegression model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# imports for randomForestRegressor model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# imports for kNeighborsRegressor model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "# other imports\n",
    "from prophet import Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pointID</th>\n",
       "      <th>unit</th>\n",
       "      <th>dqType</th>\n",
       "      <th>dqStart</th>\n",
       "      <th>dqDuration</th>\n",
       "      <th>pointInterval</th>\n",
       "      <th>his</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@p:dmc_All:r:2e223a3a-5053d2b0 DMC Building 1 ...</td>\n",
       "      <td>°C</td>\n",
       "      <td>Nulls</td>\n",
       "      <td>2023-05-09 23:55:00+04:00</td>\n",
       "      <td>2 days 00:05:00</td>\n",
       "      <td>0 days 00:05:00</td>\n",
       "      <td>Timestamp        val  \\\n",
       "0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pointID unit dqType  \\\n",
       "0  @p:dmc_All:r:2e223a3a-5053d2b0 DMC Building 1 ...   °C  Nulls   \n",
       "\n",
       "                    dqStart      dqDuration   pointInterval  \\\n",
       "0 2023-05-09 23:55:00+04:00 2 days 00:05:00 0 days 00:05:00   \n",
       "\n",
       "                                                 his  \n",
       "0                       Timestamp        val  \\\n",
       "0...  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_table = extract_dummy_data(\"dummy_data\")\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #---------\n",
    "# # TESTING for running a single model\n",
    "# #---------\n",
    "\n",
    "# # Testing variables\n",
    "# row = master_table.iloc[0]\n",
    "# df = row[\"his\"]\n",
    "# df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "# length_of_missing_data = row[\"dqDuration\"]\n",
    "# data_logging_interval = row[\"pointInterval\"]\n",
    "# dqStart\t= row['dqStart']\n",
    "# dqDuration = row['dqDuration']\n",
    "# #dqStart = dqStart - length_of_missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Working Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_naive(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) #+ 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        SeasonalNaive(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"SeasonalNaive\":\"seasonalNaive\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) #+ 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        DOT(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(df, length_of_missing_data, data_logging_interval, dqStart, featureNumber):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    dqStart: start of the predictions\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop all NaN\n",
    "    # df = df.dropna()\n",
    "\n",
    "    # Splitting variables\n",
    "    y = df[df.columns[0]]  # independent variable\n",
    "    X = df[[df.columns[featureNumber+1]]]  # dependent variable\n",
    "\n",
    "    # Filter data for training and testing\n",
    "    X_train = X[X.index < dqStart]\n",
    "    y_train = y[X.index < dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    #y_test = y[X.index >= dqStart]\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree = 4)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # Train polynomial regression model on the whole dataset\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    # Create a new DataFrame with the timestamp as index and y_pred as values\n",
    "    pred_df = pd.DataFrame(data=y_pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_Forest_Regressor(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    master_table: main table received from SS\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing columns. Index names as ts\n",
    "    \"\"\"\n",
    "    X = df.iloc[:,1:-1]\n",
    "    y = df.iloc[:,0:1]  \n",
    "\n",
    "    X_train = X[X.index < dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    y_train = y[y.index < dqStart]\n",
    "\n",
    "    # Fitting Random Forest Regression to the dataset\n",
    "    regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)\n",
    "    \n",
    "    # Fit the regressor with x and y data\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred = regressor.predict(X_test)\n",
    "    predictions = pd.DataFrame(data=pred, index=X_test.index, columns=['y_pred'])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNeighbors_Regressor_Uniform(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df:data table from SS with the ts as index\n",
    "    dqStart: start datetime\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing data inclusive of the start date and end date. Index names as ts\n",
    "\n",
    "    #Uniform: gives each data point equal weight\n",
    "    \"\"\"\n",
    "    X = df.iloc[:,1:-1]\n",
    "    y = df.iloc[:,0:1]  \n",
    "\n",
    "    X_train = X[X.index < dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    y_train = y[y.index < dqStart]\n",
    "\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=3,weights=\"uniform\")\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    pred = knn_regressor.predict(X_test)\n",
    "    predictions = pd.DataFrame(data=pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Models to be integrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "\n",
    "    # # renaming columns\n",
    "    df.columns = ['ds', 'temp']\n",
    "\n",
    "    # Remove ' Dubai' from the datetime strings\n",
    "    df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "    # Try converting the 'ds' column to datetime format with error handling\n",
    "    try:\n",
    "        df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing datetime: {e}\")\n",
    "        print(\"Some datetime strings could not be parsed. Check your data.\")\n",
    "        problematic_rows = df[pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\", errors='coerce').isna()]\n",
    "        print(\"Problematic rows:\")\n",
    "        print(problematic_rows)\n",
    "        return None\n",
    "\n",
    "    # Drop rows where datetime parsing failed\n",
    "    df = df.dropna(subset=['ds'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    # Separate data for temperature\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    # df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                         interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                         changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(df_temp)\n",
    "\n",
    "    # Calculate number of predictions\n",
    "    samples = int(length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Specify the start date for prediction\n",
    "    # dq_start = pd.Timestamp(dqStart, tz='Asia/Dubai').tz_localize(None)\n",
    "    dq_start = dqStart\n",
    "\n",
    "    # Create future DataFrame starting from dq_start\n",
    "    future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "    # Adjust 'ds' column to start from dq_start\n",
    "    future_temp['ds'] = dq_start + pd.to_timedelta(range(len(future_temp)), unit='m')\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    future_temp['ds'] = future_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "    # Ensure 'ds' column in forecast_temp is timezone-naive\n",
    "    forecast_temp['ds'] = forecast_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Convert dq_start to timezone-naive\n",
    "    dq_start = dq_start.tz_localize(None)\n",
    "\n",
    "    # Filter predictions to start from dq_start\n",
    "    predictions = forecast_temp[forecast_temp['ds'] >= dq_start][['ds', 'yhat']]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_1(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['ds', 'temp']\n",
    "\n",
    "    # Remove ' Dubai' from the datetime strings\n",
    "    df['ds'] = df['ds'].astype(str).str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "    # Convert the 'ds' column to datetime format\n",
    "    df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "    # Drop rows where datetime parsing failed\n",
    "    df = df.dropna(subset=['ds'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df['ds'] = df['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Extract numerical features from datetime\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['day'] = df['ds'].dt.day\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "    df['minute'] = df['ds'].dt.minute\n",
    "\n",
    "    # Create future DataFrame starting from dq_start\n",
    "    future_periods = int(length_of_missing_data / data_logging_interval) + 1\n",
    "    dq_start = pd.Timestamp(dqStart).tz_convert('Asia/Dubai').tz_localize(None)\n",
    "    future_temp = pd.DataFrame()\n",
    "    future_temp['ds'] = [dq_start + timedelta(minutes=5 * i) for i in range(future_periods)]\n",
    "\n",
    "    # Extract features for XGBoost\n",
    "    future_temp['year'] = future_temp['ds'].dt.year\n",
    "    future_temp['month'] = future_temp['ds'].dt.month\n",
    "    future_temp['day'] = future_temp['ds'].dt.day\n",
    "    future_temp['hour'] = future_temp['ds'].dt.hour\n",
    "    future_temp['minute'] = future_temp['ds'].dt.minute\n",
    "\n",
    "    # Initialize XGBoost model\n",
    "    model_temp = xgb.XGBRegressor()\n",
    "\n",
    "    # Fit the model\n",
    "    model_temp.fit(df[['year', 'month', 'day', 'hour', 'minute']], df['y'])\n",
    "\n",
    "    # Predict the future values\n",
    "    future_temp['yhat'] = model_temp.predict(future_temp[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "    # Filter predictions to start from dq_start\n",
    "    predictions = future_temp[['ds', 'yhat']]\n",
    "\n",
    "    # Set 'ds' as the index\n",
    "    predictions.set_index('ds', inplace=True)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_2(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = ['ds', 'temp']\n",
    "\n",
    "    # Remove ' Dubai' from the datetime strings\n",
    "    df['ds'] = df['ds'].astype(str).str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "    # Convert the 'ds' column to datetime format\n",
    "    df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "    # Drop rows where datetime parsing failed\n",
    "    df = df.dropna(subset=['ds'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    dq_start = pd.Timestamp(dqStart).tz_convert('Asia/Dubai').tz_localize(None)\n",
    "\n",
    "    # Extract numerical features from datetime\n",
    "    df['year'] = df['ds'].dt.year\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['day'] = df['ds'].dt.day\n",
    "    df['hour'] = df['ds'].dt.hour\n",
    "    df['minute'] = df['ds'].dt.minute\n",
    "\n",
    "    # Create future DataFrame starting from dq_start\n",
    "    future_periods = int(length_of_missing_data / data_logging_interval) + 1\n",
    "    dq_start = pd.Timestamp(dqStart, tz='Asia/Dubai').tz_localize(None)\n",
    "    future_temp = pd.DataFrame()\n",
    "    future_temp['ds'] = [dq_start + timedelta(minutes=5 * i) for i in range(future_periods)]\n",
    "\n",
    "    # Extract features for XGBoost\n",
    "    future_temp['year'] = future_temp['ds'].dt.year\n",
    "    future_temp['month'] = future_temp['ds'].dt.month\n",
    "    future_temp['day'] = future_temp['ds'].dt.day\n",
    "    future_temp['hour'] = future_temp['ds'].dt.hour\n",
    "    future_temp['minute'] = future_temp['ds'].dt.minute\n",
    "\n",
    "    ## Initialize XGBoost model with parameters to reduce noise\n",
    "    model_temp = xgb.XGBRegressor(\n",
    "        n_estimators=100,   # Number of boosting rounds\n",
    "        max_depth=3,        # Maximum depth of each tree\n",
    "        learning_rate=0.1,  # Learning rate\n",
    "        min_child_weight=1, # Minimum sum of instance weight needed in a child\n",
    "        subsample=0.8,      # Subsample ratio of the training instances\n",
    "        colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "        objective='reg:squarederror'  # Objective function for regression task\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    model_temp.fit(df[['year', 'month', 'day', 'hour', 'minute']], df['y'])\n",
    "\n",
    "    # Predict the future values\n",
    "    future_temp['yhat'] = model_temp.predict(future_temp[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "    # Filter predictions to start from dq_start\n",
    "    predictions = future_temp[['ds', 'yhat']]\n",
    "\n",
    "    # Set 'ds' as the index\n",
    "    predictions.set_index('ds', inplace=True)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(python_master_table, filter=True):\n",
    "    \"\"\"\n",
    "    Function to run all models, and return the one with lowest RMSE.\n",
    "    Models running through the ensemble model will have input DataFrame (AKA the \"his\" column on master_table) \n",
    "    with timestamp as index, target variable as first column, feature variables as the rest of the columns.\n",
    "\n",
    "    Make sure the output predictions of all models are INCLUSIVE of both the \"start ts\" and \"end ts\" (AKA\n",
    "    last ts with real data before gap, and first ts with real data after gap) \n",
    "\n",
    "    Make sure to follow camelCase for DataFrame column naming for compatibility with SS\n",
    "\n",
    "    filter = True. Parameter to show all predictions for each dq issue, or only ones filtered with rmse and mape.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # dictionary to save predictions for each point\n",
    "    scores_df_dict = {\n",
    "    \"pointID\": [],\n",
    "    \"predictions\": [],\n",
    "    \"rmse\": [],\n",
    "    \"mape\": [],\n",
    "    \"modelName\": []\n",
    "    }\n",
    "    # Create a DataFrame from the dictionary\n",
    "    scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "\n",
    "    for i, row in python_master_table.iterrows():\n",
    "\n",
    "        #-----------------\n",
    "        # INPUTS TO MODELS\n",
    "        #-----------------\n",
    "        # point id on SS\n",
    "        pointID = row[\"pointID\"]\n",
    "        # data logging interval of the point \n",
    "        data_logging_interval = row[\"pointInterval\"]\n",
    "        freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "        # the history df containing training data for the models\n",
    "        df = row[\"his\"]#.to_dataframe()                                      #### IMPORTANT : UNCOMMENT THIS ON SS  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "        df = df.asfreq(freq)\n",
    "        # dqStart and duration of dq issue (dqEnd = dqStart+length_of_missing_data)\n",
    "        dqStart = row[\"dqStart\"]\n",
    "        length_of_missing_data = row[\"dqDuration\"]\n",
    "\n",
    "\n",
    "        # ----------------------\n",
    "        # Training/Testing sets \n",
    "        # ----------------------\n",
    "        data_before_gap = df.loc[:dqStart][:-1] # :-1 to not include the first ts with nulls, since df.loc[:dqStart] will include the first ts with NAN\n",
    "        # doing a bfill and ffill to make sure there are no nulls in the training data. This step should be taken care of on SS by interpolating. This is done as a cautionary measure as nulls in the data will cause some models to fail\n",
    "        data_before_gap.bfill(inplace=True)\n",
    "        data_before_gap.ffill(inplace=True)\n",
    "        train_data = data_before_gap.loc[:dqStart-length_of_missing_data][:-1]\n",
    "        test_data = data_before_gap.loc[dqStart-length_of_missing_data:dqStart]\n",
    "        # test_data timestamps used to slice the predictions_for_rmse df to have exact dimension as testing set. This prevents raising error due to mismatching lengths when using the rmse or mape functions\n",
    "        test_data_timestamps = pd.date_range(start=test_data.index[0], end=test_data.index[-1], freq=test_data.index.freq)\n",
    "\n",
    "\n",
    "        # ----------------------------\n",
    "        # Timestamps for null duration\n",
    "        # ----------------------------\n",
    "        start = row['dqStart']\n",
    "        duration = row['dqDuration']\n",
    "        interval = row['pointInterval']\n",
    "        timestamps = pd.date_range(start=start, end=start+duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "\n",
    "        #----------------------------\n",
    "        # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE ################\n",
    "        #----------------------------\n",
    "\n",
    "        # UNIVARIATE Models\n",
    "        dq_models_univariate = {\n",
    "            \"Seasonal Naive\" : seasonal_naive,\n",
    "            \"Dynamic Optimized Theta\": dynamic_optimized_theta\n",
    "        }\n",
    "\n",
    "        # MULTIVARIATE Models using one feature at a time\n",
    "        dq_models_multivariate_1feature = {\n",
    "            \"Polynomial Regression\" : polynomial_regression,\n",
    "        }\n",
    "\n",
    "        # MULTIVARIATE Models using all features to predict target\n",
    "        dq_models_multivariate = {\n",
    "            \"Random Forest Regressor\" : random_Forest_Regressor,\n",
    "            \"KNN Regressor Uniform \": kNeighbors_Regressor_Uniform,\n",
    "            # \"XGBoost 1\": xgboost_1,\n",
    "            # \"XGboost 2\": xgboost_2,\n",
    "            # \"XGboost 3\": xgboost_3\n",
    "        }\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # LOOP FOR UNIVARIATE MODELS\n",
    "        ############################\n",
    "        for model_name, model in dq_models_univariate.items():\n",
    "\n",
    "            #------------------------\n",
    "            # ** Calculating RMSE **\n",
    "            #------------------------\n",
    "            # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "            predictions_for_rmse = model(train_data, length_of_missing_data, data_logging_interval, dqStart)\n",
    "            predictions_for_rmse = predictions_for_rmse[predictions_for_rmse.index.isin(test_data_timestamps)]\n",
    "            rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "            mape_score = mean_absolute_percentage_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy())\n",
    "\n",
    "\n",
    "            #------------------\n",
    "            # ** Predictions **\n",
    "            #------------------\n",
    "            # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "            predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "            # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "            predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "            # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "            predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "            # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "            predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "            # append data to the scores DF\n",
    "            row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                                \"rmse\": rmse_score, \"mape\": mape_score,\n",
    "                                \"modelName\": model_name, \n",
    "                                \"identifier\": \n",
    "                                    str(row[\"pointID\"])\n",
    "                                    +str(row[\"dqStart\"])\n",
    "                                    +str(row[\"dqDuration\"])\n",
    "                                    +str(row[\"dqType\"])}\n",
    "\n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "\n",
    "        ##############################\n",
    "        # Loop for MULTIVARIATE MODELS\n",
    "        ##############################\n",
    "        if len(df.columns)>1:  # only run multivariate if there are features available to use from the master table\n",
    "            \n",
    "            ##############\n",
    "            # Multivariate models where all features are used to predict the target\n",
    "            for model_name, model in dq_models_multivariate.items():\n",
    "                #------------------------\n",
    "                # ** Calculating RMSE **\n",
    "                #------------------------\n",
    "                # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "                predictions_for_rmse = model(data_before_gap, length_of_missing_data, data_logging_interval, dqStart-length_of_missing_data)\n",
    "                predictions_for_rmse = predictions_for_rmse[predictions_for_rmse.index.isin(test_data_timestamps)]  \n",
    "                rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "                mape_score = mean_absolute_percentage_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy())\n",
    "\n",
    "                #------------------\n",
    "                # ** Predictions **\n",
    "                #------------------\n",
    "                # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "                predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "                # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "                predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]   # UNCOMMENT THIS ONCE THE interval of the data column in SS master table is corrected\n",
    "\n",
    "                # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "                predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "                # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "                predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "                # append data to the scores DF\n",
    "                row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                                \"rmse\": rmse_score, \"mape\": mape_score,\n",
    "                                \"modelName\": model_name, \n",
    "                                \"identifier\": \n",
    "                                    str(row[\"pointID\"])\n",
    "                                    +str(row[\"dqStart\"])\n",
    "                                    +str(row[\"dqDuration\"])\n",
    "                                    +str(row[\"dqType\"])}\n",
    "\n",
    "                scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "            ##############\n",
    "            # Multivariate models where 1 feature is used at a time to predict the target\n",
    "            for model_name, model in dq_models_multivariate_1feature.items():\n",
    "                    \n",
    "                    # Loop over different features to be used one at a time\n",
    "                    for featureNumber, featureName in enumerate(df.columns.tolist()[1:]):\n",
    "\n",
    "                        #------------------------\n",
    "                        # ** Calculating RMSE **\n",
    "                        #------------------------\n",
    "                        # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "                        predictions_for_rmse = model(data_before_gap, length_of_missing_data, data_logging_interval, dqStart-length_of_missing_data, featureNumber)\n",
    "                        predictions_for_rmse = predictions_for_rmse[predictions_for_rmse.index.isin(test_data_timestamps)]  \n",
    "                        rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "                        mape_score = mean_absolute_percentage_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy())\n",
    "\n",
    "                        #------------------\n",
    "                        # ** Predictions **\n",
    "                        #------------------\n",
    "                        # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "                        predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart, featureNumber)\n",
    "\n",
    "                        # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "                        predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "                        # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "                        predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "                        # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "                        predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "                        # append data to the scores DF\n",
    "                        row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                                        \"rmse\": rmse_score, \"mape\": mape_score,\n",
    "                                        \"modelName\": model_name+\" - Feature: \" + str(featureName), \n",
    "                                        \"identifier\": \n",
    "                                            str(row[\"pointID\"])\n",
    "                                            +str(row[\"dqStart\"])\n",
    "                                            +str(row[\"dqDuration\"])\n",
    "                                            +str(row[\"dqType\"])}\n",
    "\n",
    "                        scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "    # filtering on RMSE and MAPE\n",
    "    if filter:\n",
    "        # keep only predictions with mean absolute percentage error <10%\n",
    "        scores_df = scores_df[scores_df.mape < 0.1]\n",
    "\n",
    "        # return predictions with least RMSE for each point/dq issue\n",
    "        idx = scores_df.groupby('identifier')['rmse'].idxmin()\n",
    "        scores_df = scores_df.loc[idx].reset_index(drop=True)\n",
    "\n",
    "    return scores_df.drop(columns=[\"identifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################\n",
    "# # TESTING A NEW MODEL -------------------------------------------\n",
    "# ##################################################################\n",
    "\n",
    "# dictionary to save predictions for each point\n",
    "# scores_df_dict = {\n",
    "# \"pointID\": [],\n",
    "# \"predictions\": [],\n",
    "# \"rmse\": [],\n",
    "# \"mape\": [],\n",
    "# \"modelName\": []\n",
    "# }\n",
    "# # Create a DataFrame from the dictionary\n",
    "# scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "\n",
    "\n",
    "# ## LOOP HERE @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# # for i, row in python_master_table.iterrows():\n",
    "# i = 0\n",
    "# row = master_table.iloc[i]\n",
    "\n",
    "\n",
    "# #-----------------\n",
    "# # INPUTS TO MODELS\n",
    "# #-----------------\n",
    "# # point id on SS\n",
    "# pointID = row[\"pointID\"]\n",
    "# # data logging interval of the point \n",
    "# data_logging_interval = row[\"pointInterval\"]\n",
    "# freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "# # the history df containing training data for the models\n",
    "# df = row[\"his\"]#.to_dataframe()                                      #### IMPORTANT : UNCOMMENT THIS ON SS  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "# df = df.asfreq(freq)\n",
    "# ### FOR TESTING; creating a fake dqStart and fake length of missing data for testing\n",
    "# # df.loc[df.index >= '2023-03-17', 'val'] = np.nan\n",
    "# # dqStart = df[df.val.isna()].iloc[0].name\n",
    "# # length_of_missing_data = df[df.val.isna()].iloc[-1].name - df[df.val.isna()].iloc[0].name\n",
    "# ### FOR TESTING; keeping only the df.val column for testing\n",
    "# # df = df[['val']]\n",
    "# dqStart = row[\"dqStart\"]\n",
    "# length_of_missing_data = row[\"dqDuration\"]\n",
    "\n",
    "\n",
    "# # ----------------------\n",
    "# # Training/Testing sets \n",
    "# # ----------------------\n",
    "# data_before_gap = df.loc[:dqStart][:-1] # :-1 to not include the first ts with nulls, since df.loc[:dqStart] will include the first ts with NAN\n",
    "# # doing a bfill and ffill to make sure there are no nulls in the training data. This step should be taken care of on SS by interpolating. This is done as a cautionary measure as nulls in the data will cause some models to fail\n",
    "# data_before_gap.bfill(inplace=True)\n",
    "# data_before_gap.ffill(inplace=True)\n",
    "# train_data = data_before_gap.loc[:dqStart-length_of_missing_data][:-1]\n",
    "# test_data = data_before_gap.loc[dqStart-length_of_missing_data:dqStart]\n",
    "# # test_data timestamps used to slice the predictions_for_rmse df to have exact dimension as testing set. This prevents raising error due to mismatching lengths when using the rmse or mape functions\n",
    "# test_data_timestamps = pd.date_range(start=test_data.index[0], end=test_data.index[-1], freq=test_data.index.freq)\n",
    "\n",
    "\n",
    "# # ----------------------------\n",
    "# # Timestamps for null duration\n",
    "# # ----------------------------\n",
    "# start = row['dqStart']\n",
    "# duration = row['dqDuration']\n",
    "# interval = row['pointInterval']\n",
    "# timestamps = pd.date_range(start=start, end=start+duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "\n",
    "# #----------------------------\n",
    "# # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE \n",
    "# #----------------------------\n",
    "\n",
    "# # UNIVARIATE Models\n",
    "# dq_models_univariate = {\n",
    "#     \"Seasonal Naive\" : seasonal_naive,\n",
    "#     \"Dynamic Optimized Theta\": dynamic_optimized_theta\n",
    "# }\n",
    "\n",
    "# # MULTIVARIATE Models using one feature at a time\n",
    "# dq_models_multivariate_1feature = {\n",
    "#     \"Polynomial Regression\" : polynomial_regression,\n",
    "# }\n",
    "\n",
    "# # MULTIVARIATE Models using all features to predict target\n",
    "# dq_models_multivariate = {\n",
    "#     \"Random Forest Regressor\" : random_Forest_Regressor,\n",
    "#     \"KNN Regressor Uniform \": kNeighbors_Regressor_Uniform,\n",
    "#     # \"XGBoost 1\": xgboost_1,\n",
    "#     # \"XGboost 2\": xgboost_2,\n",
    "#     # \"XGboost 3\": xgboost_3\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# ############################\n",
    "# # LOOP FOR UNIVARIATE MODELS\n",
    "# ############################\n",
    "# ## loop here @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "# # for model_name, model in dq_models_univariate.items():\n",
    "# model_name = \"Seasonal Naive\"\n",
    "# model = seasonal_naive\n",
    "\n",
    "\n",
    "# #------------------------\n",
    "# # ** Calculating RMSE **\n",
    "# #------------------------\n",
    "# # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "# predictions_for_rmse = model(data_before_gap, length_of_missing_data, data_logging_interval, dqStart-length_of_missing_data)\n",
    "# predictions_for_rmse = predictions_for_rmse[predictions_for_rmse.index.isin(test_data_timestamps)]  \n",
    "# rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "# mape_score = mean_absolute_percentage_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy())\n",
    "\n",
    "# #------------------\n",
    "# # ** Predictions **\n",
    "# #------------------\n",
    "# # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "# predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "# # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "# # predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]   # UNCOMMENT THIS ONCE THE interval of the data column in SS master table is corrected\n",
    "\n",
    "# # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "# predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "# # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "# predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "# # append data to the scores DF\n",
    "# row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "#                 \"rmse\": rmse_score, \"mape\": mape_score,\n",
    "#                 \"modelName\": model_name, \n",
    "#                 \"identifier\": \n",
    "#                     str(row[\"pointID\"])\n",
    "#                     +str(row[\"dqStart\"])\n",
    "#                     +str(row[\"dqDuration\"])\n",
    "#                     +str(row[\"dqType\"])}\n",
    "\n",
    "# scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "# scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\statsforecast\\core.py:417: FutureWarning: In a future version the predictions will have the id as a column. You can set the `NIXTLA_ID_AS_COL` environment variable to adopt the new behavior and to suppress this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\abbayoumi\\AppData\\Local\\Temp\\ipykernel_12000\\852757299.py:20: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  regressor.fit(X_train, y_train)\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:583: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n",
      "C:\\Users\\abbayoumi\\AppData\\Local\\Temp\\ipykernel_12000\\852757299.py:20: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  regressor.fit(X_train, y_train)\n",
      "c:\\Users\\abbayoumi\\AppData\\Local\\anaconda3\\envs\\env_full\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:583: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "ens = ensemble_model(master_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pointID</th>\n",
       "      <th>predictions</th>\n",
       "      <th>rmse</th>\n",
       "      <th>modelName</th>\n",
       "      <th>mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@p:dmc_All:r:2e223a3a-5053d2b0 DMC Building 1 ...</td>\n",
       "      <td>ts  predictions\n",
       "0  ...</td>\n",
       "      <td>0.698491</td>\n",
       "      <td>Polynomial Regression - Feature: discharge_Fan...</td>\n",
       "      <td>0.027129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pointID  \\\n",
       "0  @p:dmc_All:r:2e223a3a-5053d2b0 DMC Building 1 ...   \n",
       "\n",
       "                                         predictions      rmse  \\\n",
       "0                             ts  predictions\n",
       "0  ...  0.698491   \n",
       "\n",
       "                                           modelName      mape  \n",
       "0  Polynomial Regression - Feature: discharge_Fan...  0.027129  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
