{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea5ee7-1cd5-4b0f-89ba-87a8c23e05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prophet_forecasting(file_path, periods=200, freq='5T'):\n",
    "    \"\"\"\n",
    "    Function to perform time series forecasting using Prophet.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: str, path to the CSV file containing the data\n",
    "    - periods: int, number of periods to forecast\n",
    "    - freq: str, frequency of the data (e.g., '5T' for 5 minutes)\n",
    "\n",
    "    Returns:\n",
    "    - combined_temp: DataFrame, original and forecasted values for temperature\n",
    "    - combined_new_point: DataFrame, original and forecasted values for new_point\n",
    "    - rmse_temp: float, root mean squared error for temperature predictions\n",
    "    - rmse_new_point: float, root mean squared error for new_point predictions\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    data.columns = ['ts', 'temp', 'new_point']\n",
    "\n",
    "    # Convert 'ts' column to datetime and handle timezone offset\n",
    "    data['ts'] = pd.to_datetime(data['ts'].str.replace(' Dubai', ''), errors='coerce')\n",
    "    # Drop rows where datetime parsing failed\n",
    "    data = data.dropna(subset=['ts'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    data['temp'] = data['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Clean percentage column and convert to numeric\n",
    "    data['new_point'] = data['new_point'].str.replace('%', '').astype(float)\n",
    "\n",
    "    # Separate data for temperature and new_point\n",
    "    df_temp = data[['ts', 'temp']].rename(columns={'ts': 'ds', 'temp': 'y'})\n",
    "    df_new_point = data[['ts', 'new_point']].rename(columns={'ts': 'ds', 'new_point': 'y'})\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "    df_new_point['ds'] = df_new_point['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_temp, test_temp = train_test_split(df_temp, test_size=0.2, shuffle=False)\n",
    "    train_new_point, test_new_point = train_test_split(df_new_point, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "    model_new_point = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(train_temp)\n",
    "    model_new_point.fit(train_new_point)\n",
    "\n",
    "    # Create future DataFrames for both temp and new_point\n",
    "    future_temp = model_temp.make_future_dataframe(periods=periods, freq=freq)\n",
    "    future_new_point = model_new_point.make_future_dataframe(periods=periods, freq=freq)\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "    forecast_new_point = model_new_point.predict(future_new_point)\n",
    "\n",
    "    # Compute RMSE for temp\n",
    "    predicted_temp = forecast_temp['yhat'].values[-len(test_temp):]\n",
    "    rmse_temp = np.sqrt(mean_squared_error(test_temp['y'], predicted_temp))\n",
    "\n",
    "    # Compute RMSE for new_point\n",
    "    predicted_new_point = forecast_new_point['yhat'].values[-len(test_new_point):]\n",
    "    rmse_new_point = np.sqrt(mean_squared_error(test_new_point['y'], predicted_new_point))\n",
    "\n",
    "    # Combine the original data with the forecasted values for temp and new_point\n",
    "    combined_temp = pd.concat([df_temp, forecast_temp[['ds', 'yhat']].rename(columns={'yhat': 'y'})], axis=1)\n",
    "    combined_new_point = pd.concat([df_new_point, forecast_new_point[['ds', 'yhat']].rename(columns={'yhat': 'y'})], axis=1)\n",
    "\n",
    "    return combined_temp, combined_new_point, rmse_temp, rmse_new_point\n",
    "\n",
    "# Example usage\n",
    "file_path = r'C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\Data\\Train.csv'\n",
    "combined_temp, combined_new_point, rmse_temp, rmse_new_point = prophet_forecasting(file_path)\n",
    "\n",
    "print(f\"RMSE for temp: {rmse_temp}\")\n",
    "print(f\"RMSE for new_point: {rmse_new_point}\")\n",
    "\n",
    "# Save the combined DataFrames to separate CSV files\n",
    "save_path_temp = r'C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\Data\\fp6.csv'\n",
    "save_path_new_point = r'C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\Data\\fp7.csv'\n",
    "\n",
    "combined_temp.to_csv(save_path_temp, index=False)\n",
    "combined_new_point.to_csv(save_path_new_point, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44d28e9-84ae-4e6b-891c-ef0be80d52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Function to perform time series forecasting using Prophet.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, input data with columns ['ts', 'temp', 'new_point']\n",
    "    - length_of_missing_data: int, length of missing data\n",
    "    - data_logging_interval: int, data logging interval\n",
    "    - dqStart: not used in the function but kept for consistency with the original signature\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with predictions for all rows with missing columns. Index names as ts.\n",
    "    \"\"\"\n",
    "    df = df.at[0, \"his\"]\n",
    "    mt = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    mt[\"status\"] = mt.isna().any(axis=1)\n",
    "    mt_predict = mt[mt[\"status\"] == 1]\n",
    "    X_predict = mt_predict.iloc[:, 0:1]\n",
    "\n",
    "    # Filtered master table\n",
    "    mt_train = mt.dropna()\n",
    "\n",
    "    # Separate data for temperature and new_point\n",
    "    df_temp = mt_train[['temp']].reset_index().rename(columns={'ts': 'ds', 'temp': 'y'})\n",
    "    df_new_point = mt_train[['new_point']].reset_index().rename(columns={'ts': 'ds', 'new_point': 'y'})\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "    df_new_point['ds'] = df_new_point['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Calculate periods based on length_of_missing_data and data_logging_interval\n",
    "    periods = length_of_missing_data // data_logging_interval + 1\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_temp, test_temp = train_test_split(df_temp, test_size=0.2, shuffle=False)\n",
    "    train_new_point, test_new_point = train_test_split(df_new_point, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "    model_new_point = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(train_temp)\n",
    "    model_new_point.fit(train_new_point)\n",
    "\n",
    "    # Create future DataFrames for both temp and new_point\n",
    "    future_temp = model_temp.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "    future_new_point = model_new_point.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "    forecast_new_point = model_new_point.predict(future_new_point)\n",
    "\n",
    "    # Compute RMSE for temp\n",
    "    predicted_temp = forecast_temp['yhat'].values[-len(test_temp):]\n",
    "    rmse_temp = np.sqrt(mean_squared_error(test_temp['y'], predicted_temp))\n",
    "\n",
    "    # Compute RMSE for new_point\n",
    "    predicted_new_point = forecast_new_point['yhat'].values[-len(test_new_point):]\n",
    "    rmse_new_point = np.sqrt(mean_squared_error(test_new_point['y'], predicted_new_point))\n",
    "\n",
    "    # Making predictions on the same data or new data\n",
    "    predict_temp = model_temp.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "    predict_new_point = model_new_point.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "\n",
    "    df_temp_pred = pd.DataFrame(data=predict_temp['yhat'].values, index=X_predict.index, columns=['temp_pred'])\n",
    "    df_new_point_pred = pd.DataFrame(data=predict_new_point['yhat'].values, index=X_predict.index, columns=['new_point_pred'])\n",
    "\n",
    "    df = pd.concat([df_temp_pred, df_new_point_pred], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # Example usage\n",
    "# file_path = r'C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\Data\\Train.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "# forecast_df = prophet_forecasting(data, length_of_missing_data=200, data_logging_interval=5, dqStart=None)\n",
    "\n",
    "# print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd12dc3-ffb6-4d1e-93a0-5020c2a376a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Function to perform time series forecasting using Prophet.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, input data with columns ['ts', 'temp', 'new_point']\n",
    "    - length_of_missing_data: int, length of missing data\n",
    "    - data_logging_interval: int, data logging interval\n",
    "    - dqStart: not used in the function but kept for consistency with the original signature\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with predictions for all rows with missing columns. Index names as ts.\n",
    "    \"\"\"\n",
    "    df = df.at[0, \"his\"]\n",
    "    mt = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    mt[\"status\"] = mt.isna().any(axis=1)\n",
    "    mt_predict = mt[mt[\"status\"] == 1]\n",
    "    X_predict = mt_predict.iloc[:, 0:1]\n",
    "\n",
    "    # Filtered master table\n",
    "    mt_train = mt.dropna()\n",
    "\n",
    "    # Separate data for temperature and new_point\n",
    "    df_temp = mt_train[['temp']].reset_index().rename(columns={'ts': 'ds', 'temp': 'y'})\n",
    "    df_new_point = mt_train[['new_point']].reset_index().rename(columns={'ts': 'ds', 'new_point': 'y'})\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "    df_new_point['ds'] = df_new_point['ds'].dt.tz_localize(None)\n",
    "\n",
    "    length_of_missing_data=400\n",
    "    data_logging_interval=1\n",
    "\n",
    "    # Calculate periods based on length_of_missing_data and data_logging_interval\n",
    "    periods = (length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_temp, test_temp = train_test_split(df_temp, test_size=0.2, shuffle=False)\n",
    "    train_new_point, test_new_point = train_test_split(df_new_point, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "    model_new_point = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(train_temp)\n",
    "    model_new_point.fit(train_new_point)\n",
    "\n",
    "    # Create future DataFrames for both temp and new_point\n",
    "    future_temp = model_temp.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "    future_new_point = model_new_point.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "    forecast_new_point = model_new_point.predict(future_new_point)\n",
    "\n",
    "    # Compute RMSE for temp\n",
    "    predicted_temp = forecast_temp['yhat'].values[-len(test_temp):]\n",
    "    rmse_temp = np.sqrt(mean_squared_error(test_temp['y'], predicted_temp))\n",
    "\n",
    "    # Compute RMSE for new_point\n",
    "    predicted_new_point = forecast_new_point['yhat'].values[-len(test_new_point):]\n",
    "    rmse_new_point = np.sqrt(mean_squared_error(test_new_point['y'], predicted_new_point))\n",
    "\n",
    "    # Print RMSE values\n",
    "    print(f\"RMSE for temp: {rmse_temp}\")\n",
    "    print(f\"RMSE for new_point: {rmse_new_point}\")\n",
    "\n",
    "    # Making predictions on the same data or new data\n",
    "    predict_temp = model_temp.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "    predict_new_point = model_new_point.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "\n",
    "    df_temp_pred = pd.DataFrame(data=predict_temp['yhat'].values, index=X_predict.index, columns=['temp_pred'])\n",
    "    df_new_point_pred = pd.DataFrame(data=predict_new_point['yhat'].values, index=X_predict.index, columns=['new_point_pred'])\n",
    "\n",
    "    df = pd.concat([df_temp_pred, df_new_point_pred], axis=1)\n",
    "\n",
    "    # Print the head of the resulting DataFrame\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b4e6f1a-80dc-4e4b-8ca2-9e247ef52374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cc0964b-31a0-4cf1-8e8a-fe8154317dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r'C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\Data\\Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "588482d0-d356-45f3-ad8b-79f1975f3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_of_missing_data=400\n",
    "data_logging_interval=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d7c41f9-8f86-4d0c-a1bf-0f95702de3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prophet_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfcda772-906d-48e7-a254-94b5da2d09c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:14:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:14:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "09:14:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:14:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m data_logging_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# example value\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Call the forecasting function with these parameters\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m prophet_forecasting(pd\u001b[38;5;241m.\u001b[39mDataFrame([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhis\u001b[39m\u001b[38;5;124m'\u001b[39m: df}]), length_of_missing_data, data_logging_interval, dqStart\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(forecast_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[16], line 58\u001b[0m, in \u001b[0;36mprophet_forecasting\u001b[1;34m(df, length_of_missing_data, data_logging_interval, dqStart)\u001b[0m\n\u001b[0;32m     55\u001b[0m model_new_point\u001b[38;5;241m.\u001b[39mfit(train_new_point)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Create future DataFrames for both temp and new_point\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m future_temp \u001b[38;5;241m=\u001b[39m model_temp\u001b[38;5;241m.\u001b[39mmake_future_dataframe(periods\u001b[38;5;241m=\u001b[39mperiods, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_logging_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m future_new_point \u001b[38;5;241m=\u001b[39m model_new_point\u001b[38;5;241m.\u001b[39mmake_future_dataframe(periods\u001b[38;5;241m=\u001b[39mperiods, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_logging_interval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Predict the future values\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\prophet\\forecaster.py:1859\u001b[0m, in \u001b[0;36mProphet.make_future_dataframe\u001b[1;34m(self, periods, freq, include_history)\u001b[0m\n\u001b[0;32m   1854\u001b[0m dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(\n\u001b[0;32m   1855\u001b[0m     start\u001b[38;5;241m=\u001b[39mlast_date,\n\u001b[0;32m   1856\u001b[0m     periods\u001b[38;5;241m=\u001b[39mperiods \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# An extra in case we include start\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m     freq\u001b[38;5;241m=\u001b[39mfreq)\n\u001b[0;32m   1858\u001b[0m dates \u001b[38;5;241m=\u001b[39m dates[dates \u001b[38;5;241m>\u001b[39m last_date]  \u001b[38;5;66;03m# Drop start if equals last_date\u001b[39;00m\n\u001b[1;32m-> 1859\u001b[0m dates \u001b[38;5;241m=\u001b[39m dates[:periods]  \u001b[38;5;66;03m# Return correct number of periods\u001b[39;00m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_history:\n\u001b[0;32m   1862\u001b[0m     dates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_dates), dates))\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5371\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m getitem(key)\n\u001b[0;32m   5368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   5369\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[0;32m   5370\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m-> 5371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n\u001b[0;32m   5373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m   5374\u001b[0m     \u001b[38;5;66;03m# if we have list[bools, length=1e5] then doing this check+convert\u001b[39;00m\n\u001b[0;32m   5375\u001b[0m     \u001b[38;5;66;03m#  takes 166 µs + 2.1 ms and cuts the ndarray.__getitem__\u001b[39;00m\n\u001b[0;32m   5376\u001b[0m     \u001b[38;5;66;03m#  time below from 3.8 ms to 496 µs\u001b[39;00m\n\u001b[0;32m   5377\u001b[0m     \u001b[38;5;66;03m# if we already have ndarray[bool], the overhead is 1.4 µs or .25%\u001b[39;00m\n\u001b[0;32m   5378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), ExtensionDtype):\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5396\u001b[0m, in \u001b[0;36mIndex._getitem_slice\u001b[1;34m(self, slobj)\u001b[0m\n\u001b[0;32m   5392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_slice\u001b[39m(\u001b[38;5;28mself\u001b[39m, slobj: \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   5393\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5394\u001b[0m \u001b[38;5;124;03m    Fastpath for __getitem__ when we know we have a slice.\u001b[39;00m\n\u001b[0;32m   5395\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5396\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[slobj]\n\u001b[0;32m   5397\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_simple_new(res, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, refs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_references)\n\u001b[0;32m   5398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_engine\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache:\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:376\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[1;32m--> 376\u001b[0m result \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnion[Self, DTScalarOrNaT]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key))\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(result):\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:285\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    283\u001b[0m key \u001b[38;5;241m=\u001b[39m extract_array(key, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    284\u001b[0m key \u001b[38;5;241m=\u001b[39m check_array_indexer(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[1;32m--> 285\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray[key]\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_scalar(result):\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_box_func(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample data creation\n",
    "data = {\n",
    "    'ts': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "    'temp': np.random.randn(100),\n",
    "    'new_point': np.random.randn(100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducing some NaNs to simulate missing data\n",
    "df.loc[10:20, 'temp'] = np.nan\n",
    "df.loc[30:40, 'new_point'] = np.nan\n",
    "\n",
    "# Converting to the expected input format\n",
    "df['his'] = df.apply(lambda x: pd.DataFrame([{'ts': x['ts'], 'temp': x['temp'], 'new_point': x['new_point']}]), axis=1)\n",
    "\n",
    "# Define the values for length_of_missing_data and data_logging_interval\n",
    "length_of_missing_data = 400  # example value\n",
    "data_logging_interval = 1  # example value\n",
    "\n",
    "# Call the forecasting function with these parameters\n",
    "forecast_df = prophet_forecasting(pd.DataFrame([{'his': df}]), length_of_missing_data, data_logging_interval, dqStart=None)\n",
    "\n",
    "# Check the results\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fed15b9c-1a29-4431-9b91-7ff3facfa58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:16:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:16:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "09:16:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:16:42 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for temp: 1.031302303467329\n",
      "RMSE for new_point: 1.2904003849087367\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found NaN in column ds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m data_logging_interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# example value\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Call the forecasting function with these parameters\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m forecast_df \u001b[38;5;241m=\u001b[39m prophet_forecasting(pd\u001b[38;5;241m.\u001b[39mDataFrame([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhis\u001b[39m\u001b[38;5;124m'\u001b[39m: df}]), length_of_missing_data, data_logging_interval, dqStart\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Check the results\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(forecast_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn[28], line 78\u001b[0m, in \u001b[0;36mprophet_forecasting\u001b[1;34m(df, length_of_missing_data, data_logging_interval, dqStart)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRMSE for new_point: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_new_point\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Making predictions on the same data or new data\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m predict_temp \u001b[38;5;241m=\u001b[39m model_temp\u001b[38;5;241m.\u001b[39mpredict(X_predict\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{X_predict\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[0;32m     79\u001b[0m predict_new_point \u001b[38;5;241m=\u001b[39m model_new_point\u001b[38;5;241m.\u001b[39mpredict(X_predict\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{X_predict\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m}))\n\u001b[0;32m     81\u001b[0m df_temp_pred \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mpredict_temp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myhat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, index\u001b[38;5;241m=\u001b[39mX_predict\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_pred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\prophet\\forecaster.py:1270\u001b[0m, in \u001b[0;36mProphet.predict\u001b[1;34m(self, df, vectorized)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1269\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataframe has no rows.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1270\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_dataframe(df\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m   1272\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_trend(df)\n\u001b[0;32m   1273\u001b[0m seasonal_components \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_seasonal_components(df)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\prophet\\forecaster.py:294\u001b[0m, in \u001b[0;36mProphet.setup_dataframe\u001b[1;34m(self, df, initialize_scales)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn ds has timezone specified, which is not supported. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRemove timezone.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFound NaN in column ds.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_regressors:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df:\n",
      "\u001b[1;31mValueError\u001b[0m: Found NaN in column ds."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Function to perform time series forecasting using Prophet.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, input data with columns ['ts', 'temp', 'new_point']\n",
    "    - length_of_missing_data: int, length of missing data\n",
    "    - data_logging_interval: int, data logging interval\n",
    "    - dqStart: not used in the function but kept for consistency with the original signature\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with predictions for all rows with missing columns. Index names as ts.\n",
    "    \"\"\"\n",
    "    df = df.at[0, \"his\"]\n",
    "    mt = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    mt[\"status\"] = mt.isna().any(axis=1)\n",
    "    mt_predict = mt[mt[\"status\"] == 1]\n",
    "    X_predict = mt_predict.iloc[:, 0:1]\n",
    "\n",
    "    # Filtered master table\n",
    "    mt_train = mt.dropna()\n",
    "\n",
    "    # Separate data for temperature and new_point\n",
    "    df_temp = mt_train[['temp']].reset_index().rename(columns={'ts': 'ds', 'temp': 'y'})\n",
    "    df_new_point = mt_train[['new_point']].reset_index().rename(columns={'ts': 'ds', 'new_point': 'y'})\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "    df_new_point['ds'] = df_new_point['ds'].dt.tz_localize(None)\n",
    "\n",
    "    length_of_missing_data = 400\n",
    "    data_logging_interval = 1\n",
    "\n",
    "    # Calculate periods based on length_of_missing_data and data_logging_interval\n",
    "    periods = int(length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_temp, test_temp = train_test_split(df_temp, test_size=0.2, shuffle=False)\n",
    "    train_new_point, test_new_point = train_test_split(df_new_point, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "    model_new_point = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.01)\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(train_temp)\n",
    "    model_new_point.fit(train_new_point)\n",
    "\n",
    "    # Create future DataFrames for both temp and new_point\n",
    "    future_temp = model_temp.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "    future_new_point = model_new_point.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "    forecast_new_point = model_new_point.predict(future_new_point)\n",
    "\n",
    "    # Compute RMSE for temp\n",
    "    predicted_temp = forecast_temp['yhat'].values[-len(test_temp):]\n",
    "    rmse_temp = np.sqrt(mean_squared_error(test_temp['y'], predicted_temp))\n",
    "\n",
    "    # Compute RMSE for new_point\n",
    "    predicted_new_point = forecast_new_point['yhat'].values[-len(test_new_point):]\n",
    "    rmse_new_point = np.sqrt(mean_squared_error(test_new_point['y'], predicted_new_point))\n",
    "\n",
    "    # Print RMSE values\n",
    "    print(f\"RMSE for temp: {rmse_temp}\")\n",
    "    print(f\"RMSE for new_point: {rmse_new_point}\")\n",
    "\n",
    "    # Making predictions on the same data or new data\n",
    "    predict_temp = model_temp.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "    predict_new_point = model_new_point.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "\n",
    "    df_temp_pred = pd.DataFrame(data=predict_temp['yhat'].values, index=X_predict.index, columns=['temp_pred'])\n",
    "    df_new_point_pred = pd.DataFrame(data=predict_new_point['yhat'].values, index=X_predict.index, columns=['new_point_pred'])\n",
    "\n",
    "    df = pd.concat([df_temp_pred, df_new_point_pred], axis=1)\n",
    "\n",
    "    # Print the head of the resulting DataFrame\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sample data creation\n",
    "data = {\n",
    "    'ts': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "    'temp': np.random.randn(100),\n",
    "    'new_point': np.random.randn(100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducing some NaNs to simulate missing data\n",
    "df.loc[10:20, 'temp'] = np.nan\n",
    "df.loc[30:40, 'new_point'] = np.nan\n",
    "\n",
    "# Converting to the expected input format\n",
    "df['his'] = df.apply(lambda x: pd.DataFrame([{'ts': x['ts'], 'temp': x['temp'], 'new_point': x['new_point']}]), axis=1)\n",
    "\n",
    "# Define the values for length_of_missing_data and data_logging_interval\n",
    "length_of_missing_data = 400  # example value\n",
    "data_logging_interval = 1  # example value\n",
    "\n",
    "# Call the forecasting function with these parameters\n",
    "forecast_df = prophet_forecasting(pd.DataFrame([{'his': df}]), length_of_missing_data, data_logging_interval, dqStart=None)\n",
    "\n",
    "# Check the results\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "64084422-ca04-4780-834d-3cb0444a81f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:21:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:21:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "09:21:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "09:21:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for temp: 0.799515709598874\n",
      "RMSE for new_point: 1.1167313712157632\n",
      "            temp_pred  new_point_pred\n",
      "ts                                   \n",
      "2023-01-31 -59.896551       24.995766\n",
      "2023-02-01 -59.896547       24.995764\n",
      "2023-02-02 -59.896547       24.995764\n",
      "2023-02-03 -59.896547       24.995764\n",
      "2023-02-04 -59.896547       24.995764\n",
      "            temp_pred  new_point_pred\n",
      "ts                                   \n",
      "2023-01-31 -59.896551       24.995766\n",
      "2023-02-01 -59.896547       24.995764\n",
      "2023-02-02 -59.896547       24.995764\n",
      "2023-02-03 -59.896547       24.995764\n",
      "2023-02-04 -59.896547       24.995764\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prophet_forecasting(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Function to perform time series forecasting using Prophet.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, input data with columns ['ts', 'temp', 'new_point']\n",
    "    - length_of_missing_data: int, length of missing data\n",
    "    - data_logging_interval: int, data logging interval\n",
    "    - dqStart: not used in the function but kept for consistency with the original signature\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame with predictions for all rows with missing columns. Index names as ts.\n",
    "    \"\"\"\n",
    "    df = df.at[0, \"his\"]\n",
    "    mt = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    mt[\"status\"] = mt.isna().any(axis=1)\n",
    "    mt_predict = mt[mt[\"status\"] == 1]\n",
    "    X_predict = mt_predict.iloc[:, 0:1]\n",
    "\n",
    "    # Filtered master table\n",
    "    mt_train = mt.dropna()\n",
    "\n",
    "    # Separate data for temperature and new_point\n",
    "    df_temp = mt_train[['temp']].reset_index().rename(columns={'ts': 'ds', 'temp': 'y'})\n",
    "    df_new_point = mt_train[['new_point']].reset_index().rename(columns={'ts': 'ds', 'new_point': 'y'})\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "    df_new_point['ds'] = df_new_point['ds'].dt.tz_localize(None)\n",
    "\n",
    "        # Calculate periods based on length_of_missing_data and data_logging_interval\n",
    "    periods = int(length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_temp, test_temp = train_test_split(df_temp, test_size=0.2, shuffle=False)\n",
    "    train_new_point, test_new_point = train_test_split(df_new_point, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.00001)\n",
    "    model_new_point = Prophet(seasonality_mode='additive', interval_width=0.95, changepoint_prior_scale=0.001)\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(train_temp)\n",
    "    model_new_point.fit(train_new_point)\n",
    "\n",
    "    # Create future DataFrames for both temp and new_point\n",
    "    future_temp = model_temp.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "    future_new_point = model_new_point.make_future_dataframe(periods=periods, freq=f'{data_logging_interval}T')\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "    forecast_new_point = model_new_point.predict(future_new_point)\n",
    "\n",
    "    # Compute RMSE for temp\n",
    "    predicted_temp = forecast_temp['yhat'].values[-len(test_temp):]\n",
    "    rmse_temp = np.sqrt(mean_squared_error(test_temp['y'], predicted_temp))\n",
    "\n",
    "    # Compute RMSE for new_point\n",
    "    predicted_new_point = forecast_new_point['yhat'].values[-len(test_new_point):]\n",
    "    rmse_new_point = np.sqrt(mean_squared_error(test_new_point['y'], predicted_new_point))\n",
    "\n",
    "    # Print RMSE values\n",
    "    print(f\"RMSE for temp: {rmse_temp}\")\n",
    "    print(f\"RMSE for new_point: {rmse_new_point}\")\n",
    "\n",
    "    # Making predictions on the same data or new data\n",
    "    X_predict = X_predict.dropna()  # Remove rows with NaN values in 'ds'\n",
    "    predict_temp = model_temp.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "    predict_new_point = model_new_point.predict(X_predict.rename(columns={X_predict.columns[0]: 'ds'}))\n",
    "\n",
    "    df_temp_pred = pd.DataFrame(data=predict_temp['yhat'].values, index=X_predict.index, columns=['temp_pred'])\n",
    "    df_new_point_pred = pd.DataFrame(data=predict_new_point['yhat'].values, index=X_predict.index, columns=['new_point_pred'])\n",
    "\n",
    "    df = pd.concat([df_temp_pred, df_new_point_pred], axis=1)\n",
    "\n",
    "    # Print the head of the resulting DataFrame\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sample data creation\n",
    "data = {\n",
    "    'ts': pd.date_range(start='2023-01-01', periods=100, freq='D'),\n",
    "    'temp': np.random.randn(100),\n",
    "    'new_point': np.random.randn(100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introducing some NaNs to simulate missing data\n",
    "df.loc[10:20, 'temp'] = np.nan\n",
    "df.loc[30:40, 'new_point'] = np.nan\n",
    "\n",
    "# Converting to the expected input format\n",
    "df['his'] = df.apply(lambda x: pd.DataFrame([{'ts': x['ts'], 'temp': x['temp'], 'new_point': x['new_point']}]), axis=1)\n",
    "\n",
    "# Define the values for length_of_missing_data and data_logging_interval\n",
    "length_of_missing_data = 400  # example value\n",
    "data_logging_interval = 1  # example value\n",
    "\n",
    "# Call the forecasting function with these parameters\n",
    "forecast_df = prophet_forecasting(pd.DataFrame([{'his': df}]), length_of_missing_data, data_logging_interval, dqStart=None)\n",
    "\n",
    "# Check the results\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623249eb-e7cc-4c7d-b7fb-6e2c9edcdb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
