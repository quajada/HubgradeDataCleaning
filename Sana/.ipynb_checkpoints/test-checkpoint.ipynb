{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242fe7c2-7311-4509-a539-2ea89c174d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\data_extraction\\dummy_data_extractor.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2023-03-12 01:05:00+04:00' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  pythonDF.loc[i, 'dqStart'] = pd.to_datetime(df['ts'].iloc[i], format=\"%Y-%m-%dT%H:%M:%S%z Dubai\")\n",
      "12:57:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "12:57:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ds       yhat\n",
      "0 2023-03-14 21:10:00  24.632758\n",
      "1 2023-03-14 21:15:00  24.610346\n",
      "2 2023-03-14 21:20:00  24.587935\n",
      "3 2023-03-14 21:25:00  24.565586\n",
      "4 2023-03-14 21:30:00  24.543357\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "\n",
    "master_table = extract_dummy_data(\"dummy_data\")\n",
    "\n",
    "# Extract the relevant dataframe from 'master_table'\n",
    "df = master_table.at[1, \"his\"]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "# renaming columns\n",
    "df.columns = ['ds', 'temp']\n",
    "\n",
    "# Remove ' Dubai' from the datetime strings\n",
    "df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "# Convert the 'ds' column to datetime format\n",
    "df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# Drop rows where datetime parsing failed\n",
    "df = df.dropna(subset=['ds'])\n",
    "\n",
    "# Clean temperature column and convert to numeric\n",
    "df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "# Rename columns for convenience\n",
    "df.columns = ['ds', 'y']\n",
    "\n",
    "# Separate data for temperature\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Initialize Prophet models with tuned hyperparameters\n",
    "model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                     interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                     changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "# Fit the models\n",
    "model_temp.fit(df_temp)\n",
    "\n",
    "length_of_missing_data = pd.Timedelta('0 days 23:30:00')\n",
    "\n",
    "\n",
    "data_logging_interval = pd.Timedelta('0 days 00:05:00')\n",
    "\n",
    " # number of predictions\n",
    "samples = int(length_of_missing_data/data_logging_interval) + 1\n",
    "\n",
    "# Create future DataFrames for both temp and new_point (next 200 samples, assuming 5-minute intervals)\n",
    "future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "# Predict the future values\n",
    "forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "# Extract 'ds' and 'yhat' from forecast_temp\n",
    "prediction = forecast_temp[['ds', 'yhat']]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d723b936-bae7-42bc-8ca0-d9da4d161ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\data_extraction\\dummy_data_extractor.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2023-03-12 01:05:00+04:00' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  pythonDF.loc[i, 'dqStart'] = pd.to_datetime(df['ts'].iloc[i], format=\"%Y-%m-%dT%H:%M:%S%z Dubai\")\n",
      "15:40:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "15:40:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ds       yhat\n",
      "0 2023-03-19 01:10:00  22.503415\n",
      "1 2023-03-19 01:11:00  22.501365\n",
      "2 2023-03-19 01:12:00  22.499302\n",
      "3 2023-03-19 01:13:00  22.497226\n",
      "4 2023-03-19 01:14:00  22.495137\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "\n",
    "master_table = extract_dummy_data(\"dummy_data\")\n",
    "\n",
    "# Extract the relevant dataframe from 'master_table'\n",
    "df = master_table.at[1, \"his\"]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "# renaming columns\n",
    "df.columns = ['ds', 'temp']\n",
    "\n",
    "# Remove ' Dubai' from the datetime strings\n",
    "df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "# Convert the 'ds' column to datetime format\n",
    "df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# Drop rows where datetime parsing failed\n",
    "df = df.dropna(subset=['ds'])\n",
    "\n",
    "# Clean temperature column and convert to numeric\n",
    "df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "# Rename columns for convenience\n",
    "df.columns = ['ds', 'y']\n",
    "\n",
    "# Separate data for temperature\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "length_of_missing_data = pd.Timedelta('0 days 23:30:00')\n",
    "\n",
    "\n",
    "data_logging_interval = pd.Timedelta('0 days 00:05:00')\n",
    "\n",
    " # number of predictions\n",
    "samples = int(length_of_missing_data/data_logging_interval) + 1\n",
    "\n",
    "# Specify the start date for prediction\n",
    "dq_start = pd.Timestamp('2023-03-19 01:10:00', tz='Asia/Dubai')\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Prophet models with tuned hyperparameters\n",
    "model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                     interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                     changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "# Fit the models\n",
    "model_temp.fit(df_temp)\n",
    "\n",
    "# Create future DataFrame starting from dq_start\n",
    "future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "# Adjust 'ds' column to start from dq_start\n",
    "future_temp['ds'] = dq_start + pd.to_timedelta(range(len(future_temp)), unit='m')\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "future_temp['ds'] = future_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Predict the future values\n",
    "forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "# Ensure 'ds' column in forecast_temp is timezone-naive\n",
    "forecast_temp['ds'] = forecast_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Convert dq_start to timezone-naive\n",
    "dq_start = dq_start.tz_localize(None)\n",
    "\n",
    "# Filter predictions to start from dq_start\n",
    "prediction = forecast_temp[forecast_temp['ds'] >= dq_start][['ds', 'yhat']]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be732d9c-9ecf-4e26-ae5f-3ffb1c4788e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Remove ' Dubai' from the datetime strings\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Dubai\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Convert the 'ds' column to datetime format\u001b[39;00m\n\u001b[0;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\generic.py:6204\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   6198\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   6199\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   6200\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   6201\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   6202\u001b[0m ):\n\u001b[0;32m   6203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 6204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\accessor.py:224\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor\n\u001b[1;32m--> 224\u001b[0m accessor_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessor(obj)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:190\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstring_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate(data)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:244\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    241\u001b[0m inferred_dtype \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39minfer_dtype(values, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inferred_dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_types:\n\u001b[1;32m--> 244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only use .str accessor with string values!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "\n",
    "def facebook_prophet(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "\n",
    "    master_table = extract_dummy_data(\"dummy_data\")\n",
    "\n",
    "row = master_table.iloc[1]\n",
    "df = row[\"his\"]\n",
    "df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "length_of_missing_data = row[\"dqDuration\"]\n",
    "data_logging_interval = row[\"pointInterval\"]\n",
    "dqStart\t= row['dqStart']\n",
    "dqDuration = row['dqDuration']\n",
    "\n",
    "\n",
    "\n",
    "# Extract the relevant dataframe from 'master_table'\n",
    "df = master_table.at[1, \"his\"]\n",
    "\n",
    "# df.reset_index(inplace=False)\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "# renaming columns\n",
    "df.columns = ['ds', 'temp']\n",
    "\n",
    "# Remove ' Dubai' from the datetime strings\n",
    "df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "# Convert the 'ds' column to datetime format\n",
    "df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# Drop rows where datetime parsing failed\n",
    "df = df.dropna(subset=['ds'])\n",
    "\n",
    "# Clean temperature column and convert to numeric\n",
    "df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "# Rename columns for convenience\n",
    "df.columns = ['ds', 'y']\n",
    "\n",
    "# Separate data for temperature\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# length_of_missing_data = pd.Timedelta('0 days 23:30:00')\n",
    "\n",
    "\n",
    "# data_logging_interval = pd.Timedelta('0 days 00:05:00')\n",
    "\n",
    " # number of predictions\n",
    "samples = int(length_of_missing_data/data_logging_interval) + 1\n",
    "\n",
    "# # Specify the start date for prediction\n",
    "# dq_start = pd.Timestamp('2023-03-19 01:10:00', tz='Asia/Dubai')\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Prophet models with tuned hyperparameters\n",
    "model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                     interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                     changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "# Fit the models\n",
    "model_temp.fit(df_temp)\n",
    "\n",
    "# Create future DataFrame starting from dq_start\n",
    "future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "# Adjust 'ds' column to start from dq_start\n",
    "future_temp['ds'] = dq_start + pd.to_timedelta(range(len(future_temp)), unit='m')\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "future_temp['ds'] = future_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Predict the future values\n",
    "forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "# Ensure 'ds' column in forecast_temp is timezone-naive\n",
    "forecast_temp['ds'] = forecast_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Convert dq_start to timezone-naive\n",
    "dq_start = dq_start.tz_localize(None)\n",
    "\n",
    "# Filter predictions to start from dq_start\n",
    "prediction = forecast_temp[forecast_temp['ds'] >= dq_start][['ds', 'yhat']]\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(prediction.head())\n",
    "\n",
    "return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a97f6c45-727c-454e-9574-4283fd16afcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"24.269638061523438°C\" doesn't match format \"%Y-%m-%dT%H:%M:%S%z\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Dubai\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Convert the 'ds' column to datetime format\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Drop rows where datetime parsing failed\u001b[39;00m\n\u001b[0;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mds\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1108\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1106\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1108\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1110\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:254\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    252\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 254\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m convert_listlike(unique_dates, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:488\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m    490\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64ns(\n\u001b[0;32m    491\u001b[0m     arg,\n\u001b[0;32m    492\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    496\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    497\u001b[0m )\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:519\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[0;32m    509\u001b[0m     arg,\n\u001b[0;32m    510\u001b[0m     name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m    516\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     result, timezones \u001b[38;5;241m=\u001b[39m array_strptime(arg, fmt, exact\u001b[38;5;241m=\u001b[39mexact, errors\u001b[38;5;241m=\u001b[39merrors, utc\u001b[38;5;241m=\u001b[39mutc)\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tz \u001b[38;5;129;01min\u001b[39;00m timezones):\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _return_parsed_timezone_results(result, timezones, utc, name)\n",
      "File \u001b[1;32mstrptime.pyx:534\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mstrptime.pyx:355\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: time data \"24.269638061523438°C\" doesn't match format \"%Y-%m-%dT%H:%M:%S%z\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "\n",
    "def facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep only the first two columns\n",
    "df = df.iloc[:, :2]\n",
    "\n",
    "# renaming columns\n",
    "df.columns = ['ds', 'temp']\n",
    "\n",
    "# Remove ' Dubai' from the datetime strings\n",
    "df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "# Convert the 'ds' column to datetime format\n",
    "df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "\n",
    "# Drop rows where datetime parsing failed\n",
    "df = df.dropna(subset=['ds'])\n",
    "\n",
    "# Clean temperature column and convert to numeric\n",
    "df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "# Rename columns for convenience\n",
    "df.columns = ['ds', 'y']\n",
    "\n",
    "# Separate data for temperature\n",
    "df_temp = df.copy()\n",
    "\n",
    "# Ensure 'ds' column is timezone-naive\n",
    "df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Initialize Prophet models with tuned hyperparameters\n",
    "model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                     interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                     changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "# Fit the models\n",
    "model_temp.fit(df_temp)\n",
    "\n",
    "\n",
    " # number of predictions\n",
    "samples = int(length_of_missing_data/data_logging_interval) + 1\n",
    "\n",
    "# Create future DataFrames for both temp and new_point (next 200 samples, assuming 5-minute intervals)\n",
    "future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "# Predict the future values\n",
    "forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "# Extract 'ds' and 'yhat' from forecast_temp\n",
    "prediction = forecast_temp[['ds', 'yhat']]\n",
    "\n",
    "return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Replace these values with your actual data and variables\n",
    "master_table = extract_dummy_data(\"dummy_data\")\n",
    "df = master_table.at[1, \"his\"].iloc[:, :2].copy()\n",
    "\n",
    "length_of_missing_data = pd.Timedelta('0 days 23:30:00')\n",
    "data_logging_interval = pd.Timedelta('0 days 00:05:00')\n",
    "dqStart = '2023-03-19 01:10:00'\n",
    "\n",
    "# Call the function\n",
    "predictions = facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a9ac59b6-6c6f-41ab-8822-69ff1a795e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syounas\\OneDrive - Enova Facilities Management\\Tasks\\GitHub\\HubgradeDataCleaning\\Sana\\data_extraction\\dummy_data_extractor.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2023-03-12 01:05:00+04:00' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  pythonDF.loc[i, 'dqStart'] = pd.to_datetime(df['ts'].iloc[i], format=\"%Y-%m-%dT%H:%M:%S%z Dubai\")\n",
      "16:24:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:24:56 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ds       yhat\n",
      "0 2023-03-19 01:10:00  22.503415\n",
      "1 2023-03-19 01:11:00  22.501365\n",
      "2 2023-03-19 01:12:00  22.499302\n",
      "3 2023-03-19 01:13:00  22.497226\n",
      "4 2023-03-19 01:14:00  22.495137\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "\n",
    "def facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only the first two columns\n",
    "    df = df.iloc[:, :2]\n",
    "\n",
    "    # renaming columns\n",
    "    df.columns = ['ds', 'temp']\n",
    "\n",
    "    # Remove ' Dubai' from the datetime strings\n",
    "    df['ds'] = df['ds'].str.replace(' Dubai', '', regex=False)\n",
    "\n",
    "    # Try converting the 'ds' column to datetime format with error handling\n",
    "    try:\n",
    "        df['ds'] = pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing datetime: {e}\")\n",
    "        print(\"Some datetime strings could not be parsed. Check your data.\")\n",
    "        problematic_rows = df[pd.to_datetime(df['ds'], format=\"%Y-%m-%dT%H:%M:%S%z\", errors='coerce').isna()]\n",
    "        print(\"Problematic rows:\")\n",
    "        print(problematic_rows)\n",
    "        return None\n",
    "\n",
    "    # Drop rows where datetime parsing failed\n",
    "    df = df.dropna(subset=['ds'])\n",
    "\n",
    "    # Clean temperature column and convert to numeric\n",
    "    df['temp'] = df['temp'].str.replace('°C', '').astype(float)\n",
    "\n",
    "    # Rename columns for convenience\n",
    "    df.columns = ['ds', 'y']\n",
    "\n",
    "    # Separate data for temperature\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    df_temp['ds'] = df_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Initialize Prophet models with tuned hyperparameters\n",
    "    model_temp = Prophet(seasonality_mode='additive',     # Adjust based on data exploration\n",
    "                         interval_width=0.95,              # Adjust prediction interval if needed\n",
    "                         changepoint_prior_scale=0.01)    # Tune based on data patterns\n",
    "\n",
    "    # Fit the models\n",
    "    model_temp.fit(df_temp)\n",
    "\n",
    "    # Calculate number of predictions\n",
    "    samples = int(length_of_missing_data / data_logging_interval) + 1\n",
    "\n",
    "    # Specify the start date for prediction\n",
    "    dq_start = pd.Timestamp(dqStart, tz='Asia/Dubai').tz_localize(None)\n",
    "\n",
    "    # Create future DataFrame starting from dq_start\n",
    "    future_temp = model_temp.make_future_dataframe(periods=samples, freq='5T')\n",
    "\n",
    "    # Adjust 'ds' column to start from dq_start\n",
    "    future_temp['ds'] = dq_start + pd.to_timedelta(range(len(future_temp)), unit='m')\n",
    "\n",
    "    # Ensure 'ds' column is timezone-naive\n",
    "    future_temp['ds'] = future_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Predict the future values\n",
    "    forecast_temp = model_temp.predict(future_temp)\n",
    "\n",
    "    # Ensure 'ds' column in forecast_temp is timezone-naive\n",
    "    forecast_temp['ds'] = forecast_temp['ds'].dt.tz_localize(None)\n",
    "\n",
    "    # Convert dq_start to timezone-naive\n",
    "    dq_start = dq_start.tz_localize(None)\n",
    "\n",
    "    # Filter predictions to start from dq_start\n",
    "    predictions = forecast_temp[forecast_temp['ds'] >= dq_start][['ds', 'yhat']]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# Replace these values with your actual data and variables\n",
    "master_table = extract_dummy_data(\"dummy_data\")\n",
    "df = master_table.at[1, \"his\"].iloc[:, :2].copy()\n",
    "\n",
    "length_of_missing_data = pd.Timedelta('0 days 23:30:00')\n",
    "data_logging_interval = pd.Timedelta('0 days 00:05:00')\n",
    "dqStart = '2023-03-19 01:10:00'\n",
    "\n",
    "# Call the function\n",
    "predictions = facebook_pred(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "if predictions is not None:\n",
    "    print(predictions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eeb87-d2ab-41f9-ad21-b6db7fa9a601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
