{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from project.data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsforecast import StatsForecast\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import re\n",
    "from statsforecast.models import (\n",
    "    # HoltWinters,\n",
    "    # CrostonClassic as Croston, \n",
    "    # HistoricAverage,\n",
    "    DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive,\n",
    "    # AutoARIMA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carabbat\\OneDrive - Enova Facilities Management\\Documents\\Hubgrade\\Skyspark\\Data Imputation\\HubgradeDataCleaning\\catherine\\data_extraction\\dummy_data_extractor.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2023-03-12 01:05:00+04:00' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  pythonDF.loc[i, 'dqStart'] = pd.to_datetime(df['ts'].iloc[i], format=\"%Y-%m-%dT%H:%M:%S%z Dubai\")\n"
     ]
    }
   ],
   "source": [
    "master_table = extract_dummy_data(\"dummy_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_naive(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        SeasonalNaive(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"SeasonalNaive\":\"seasonalNaive\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        DOT(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_regression(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    dqStart: start of the predictions\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop all NaN\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Splitting variables\n",
    "    y = df[df.columns[0]]  # independent variable\n",
    "    X = df[[df.columns[1]]]  # dependent variable\n",
    "\n",
    "    # Filter data for training and testing\n",
    "    X_train = X[X.index < dqStart]\n",
    "    y_train = y[X.index < dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    #y_test = y[X.index >= dqStart]\n",
    "\n",
    "    # Generate polynomial features\n",
    "    poly = PolynomialFeatures(degree = 4)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "\n",
    "    # Train polynomial regression model on the whole dataset\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_poly)\n",
    "\n",
    "    # Create a new DataFrame with the timestamp as index and y_pred as values\n",
    "    pred_df = pd.DataFrame(data=y_pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "    return pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-10 01:05:00+04:00</th>\n",
       "      <td>23.166546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-10 01:10:00+04:00</th>\n",
       "      <td>23.166546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-10 01:15:00+04:00</th>\n",
       "      <td>23.166546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-10 01:20:00+04:00</th>\n",
       "      <td>23.166546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-10 01:25:00+04:00</th>\n",
       "      <td>23.166546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-12 00:40:00+04:00</th>\n",
       "      <td>22.934544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-12 00:45:00+04:00</th>\n",
       "      <td>23.002886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-12 00:50:00+04:00</th>\n",
       "      <td>23.144585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-12 00:55:00+04:00</th>\n",
       "      <td>23.022804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-12 01:00:00+04:00</th>\n",
       "      <td>23.022804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y_pred\n",
       "ts                                  \n",
       "2023-03-10 01:05:00+04:00  23.166546\n",
       "2023-03-10 01:10:00+04:00  23.166546\n",
       "2023-03-10 01:15:00+04:00  23.166546\n",
       "2023-03-10 01:20:00+04:00  23.166546\n",
       "2023-03-10 01:25:00+04:00  23.166546\n",
       "...                              ...\n",
       "2023-03-12 00:40:00+04:00  22.934544\n",
       "2023-03-12 00:45:00+04:00  23.002886\n",
       "2023-03-12 00:50:00+04:00  23.144585\n",
       "2023-03-12 00:55:00+04:00  23.022804\n",
       "2023-03-12 01:00:00+04:00  23.022804\n",
       "\n",
       "[576 rows x 1 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = master_table.iloc[0]\n",
    "df = row[\"his\"]#.to_dataframe()                           #### IMPORTANT : UNCOMMENT THIS ON SS\n",
    "df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "length_of_missing_data = row[\"dqDuration\"]\n",
    "data_logging_interval = row[\"pointInterval\"]\n",
    "dqStart = '2023-03-10 01:05:00+0400'\n",
    "\n",
    "#seasonal_naive(df, length_of_missing_data, data_logging_interval)\n",
    "#dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval)\n",
    "polynomial_regression(df, length_of_missing_data, data_logging_interval, dqStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(python_master_table):\n",
    "    \"\"\"\n",
    "    Function to run all models, and return the one with lowest RMSE.\n",
    "    Models running through the ensemble model will have input DataFrame (AKA the \"his\" column on master_table) \n",
    "    with timestamp as index, target variable as first column, feature variables as the rest of the columns.\n",
    "\n",
    "    Make sure the output predictions of all models are INCLUSIVE of both the \"start ts\" and \"end ts\" (AKA\n",
    "    last ts with real data before gap, and first ts with real data after gap) \n",
    "\n",
    "    Make sure to follow camelCase for DataFrame column naming for compatibility with SS\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary to save predictions for each point\n",
    "    scores_df_dict = {\n",
    "    \"pointID\": [],\n",
    "    \"predictions\": [],\n",
    "    \"rmse\": [],\n",
    "    \"modelName\": []\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "    for i, row in python_master_table.iterrows():\n",
    "\n",
    "        #-----------------\n",
    "        # INPUTS TO MODELS\n",
    "        #-----------------\n",
    "\n",
    "        pointID = row[\"pointID\"]\n",
    "        df = row[\"his\"]#.to_dataframe()                           #### IMPORTANT : UNCOMMENT THIS ON SS\n",
    "        df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "        length_of_missing_data = row[\"dqDuration\"]\n",
    "        data_logging_interval = row[\"pointInterval\"]\n",
    "        dqStart = '2023-03-10 01:05:00+0400'\n",
    "\n",
    "\n",
    "        #----------------------------\n",
    "        # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE \n",
    "        #----------------------------\n",
    "\n",
    "        dq_models = {\n",
    "            \"Seasonal Naive\" : seasonal_naive,\n",
    "            \"Dynamic Optimized Theta\": dynamic_optimized_theta,\n",
    "            \"Polynomial Regression\": polynomial_regression\n",
    "        }\n",
    "\n",
    "        for model_name, model in dq_models.items():\n",
    "            \n",
    "            #------------------------\n",
    "            # ** Calculating RMSE **\n",
    "            #------------------------\n",
    "\n",
    "            # number of predictions needed\n",
    "            horizon = int(length_of_missing_data/data_logging_interval) +1 # why +1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions INCLUSIVE of BOTH start and end ts\n",
    "\n",
    "            # training set size (relative to the horizon/prediction size)\n",
    "            training_set_size = horizon * 10\n",
    "\n",
    "            # training / testing set to evaluate the model (relative to horizon of prediction)\n",
    "            train_data = df.iloc[-1*int(training_set_size):-1*int(horizon)]\n",
    "            test_data = df.iloc[-1*int(horizon):]\n",
    "\n",
    "            # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "            predictions_for_rmse = model(df = train_data, length_of_missing_data = length_of_missing_data, data_logging_interval = data_logging_interval, dqStart = dqStart)\n",
    "            rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "\n",
    "            #------------------\n",
    "            # ** Predictions **\n",
    "            #------------------\n",
    "\n",
    "            # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "            predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval)\n",
    "\n",
    "            # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "            start = row['dqStart']\n",
    "            duration = row['dqDuration']\n",
    "            interval = row['pointInterval']\n",
    "            timestamps = pd.date_range(start=start, end=start + duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "            predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "            # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "            predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "            # append data to the scores DF\n",
    "            row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                            \"rmse\": rmse_score, \"modelName\": model_name, \n",
    "                            \"identifier\": \n",
    "                                str(row[\"pointID\"])\n",
    "                                +str(row[\"dqStart\"])\n",
    "                                +str(row[\"dqDuration\"])\n",
    "                                +str(row[\"dqType\"])}\n",
    "            \n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "            # return predictions with least RMSE for each point/dq issue\n",
    "            idx = scores_df.groupby('identifier')['rmse'].idxmin()\n",
    "            scores_df = scores_df.loc[idx].reset_index(drop=True)\n",
    "            \n",
    "    return scores_df    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ds' should have valid timestamps or integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[176], line 65\u001b[0m, in \u001b[0;36mensemble_model\u001b[1;34m(python_master_table)\u001b[0m\n\u001b[0;32m     62\u001b[0m test_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(horizon):]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# the prediction. USED ONLY TO EVALUATE RMSE\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m predictions_for_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_of_missing_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength_of_missing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_logging_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_logging_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdqStart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdqStart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m rmse_score \u001b[38;5;241m=\u001b[39m mean_squared_error(test_data[test_data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy(), predictions_for_rmse[predictions_for_rmse\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy(), squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#------------------\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# ** Predictions **\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m#------------------\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# the predictions. USED FOR DATA CLEANING (uses all the data as training)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[172], line 50\u001b[0m, in \u001b[0;36mseasonal_naive\u001b[1;34m(df, length_of_missing_data, data_logging_interval, dqStart)\u001b[0m\n\u001b[0;32m     42\u001b[0m sf \u001b[38;5;241m=\u001b[39m StatsForecast( \n\u001b[0;32m     43\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m     44\u001b[0m     freq\u001b[38;5;241m=\u001b[39mfreq, \n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# fallback_model = SeasonalNaive(season_length=season_length),\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Model fitting\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m forecasts_df \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munique_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# removing the -hi- and -lo- columns\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m forecasts_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\carabbat\\.conda\\envs\\dataCleaning1\\lib\\site-packages\\statsforecast\\core.py:1622\u001b[0m, in \u001b[0;36mStatsForecast.forecast\u001b[1;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must specify `level` when using `prediction_intervals`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1620\u001b[0m     )\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_native(df\u001b[38;5;241m=\u001b[39mdf):\n\u001b[1;32m-> 1622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfitted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_intervals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m engine \u001b[38;5;241m=\u001b[39m make_execution_engine(infer_by\u001b[38;5;241m=\u001b[39m[df])\n",
      "File \u001b[1;32mc:\\Users\\carabbat\\.conda\\envs\\dataCleaning1\\lib\\site-packages\\statsforecast\\core.py:918\u001b[0m, in \u001b[0;36m_StatsForecast.forecast\u001b[1;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m\"\"\"Memory Efficient predictions.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03mThis method avoids memory burden due from object storage.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    predictions for all fitted `models`.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfcst_fitted_values_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 918\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_exog(X_df)\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sizes_for_prediction_intervals(prediction_intervals)\n",
      "File \u001b[1;32mc:\\Users\\carabbat\\.conda\\envs\\dataCleaning1\\lib\\site-packages\\statsforecast\\core.py:616\u001b[0m, in \u001b[0;36m_StatsForecast._prepare_fit\u001b[1;34m(self, df, sort_df, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m    614\u001b[0m     _warn_df_constructor()\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mensure_time_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m validate_freq(df[time_col], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, pd\u001b[38;5;241m.\u001b[39mDataFrame) \u001b[38;5;129;01mand\u001b[39;00m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m id_col:\n",
      "File \u001b[1;32mc:\\Users\\carabbat\\.conda\\envs\\dataCleaning1\\lib\\site-packages\\utilsforecast\\validation.py:71\u001b[0m, in \u001b[0;36mensure_time_dtype\u001b[1;34m(df, time_col)\u001b[0m\n\u001b[0;32m     69\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(times)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should have valid timestamps or integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mValueError\u001b[0m: 'ds' should have valid timestamps or integers."
     ]
    }
   ],
   "source": [
    "a = ensemble_model(master_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pointID</th>\n",
       "      <th>predictions</th>\n",
       "      <th>rmse</th>\n",
       "      <th>modelName</th>\n",
       "      <th>identifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "      <td>timestamp  seasonalNaive\n",
       "0...</td>\n",
       "      <td>0.141386</td>\n",
       "      <td>Seasonal Naive</td>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "      <td>timestamp  dynamicOptimize...</td>\n",
       "      <td>0.574149</td>\n",
       "      <td>Dynamic Optimized Theta</td>\n",
       "      <td>@p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             pointID  \\\n",
       "0  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...   \n",
       "1  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...   \n",
       "\n",
       "                                         predictions      rmse  \\\n",
       "0                      timestamp  seasonalNaive\n",
       "0...  0.141386   \n",
       "1                      timestamp  dynamicOptimize...  0.574149   \n",
       "\n",
       "                 modelName                                         identifier  \n",
       "0           Seasonal Naive  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...  \n",
       "1  Dynamic Optimized Theta  @p:dmc_All:r:2ddf07d5-ef59ca94 DMC Building 1 ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
