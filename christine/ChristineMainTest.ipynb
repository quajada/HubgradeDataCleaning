{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy</th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-05-01 00:00:00+04:00</td>\n",
       "      <td>18.262186</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-05-01 00:05:00+04:00</td>\n",
       "      <td>20.425119</td>\n",
       "      <td>0.1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-05-01 00:10:00+04:00</td>\n",
       "      <td>15.942257</td>\n",
       "      <td>0.2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-01 00:15:00+04:00</td>\n",
       "      <td>15.942257</td>\n",
       "      <td>0.2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-01 00:20:00+04:00</td>\n",
       "      <td>15.792557</td>\n",
       "      <td>0.2525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8922</th>\n",
       "      <td>2023-05-31 23:35:00+04:00</td>\n",
       "      <td>13.669825</td>\n",
       "      <td>0.4756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8923</th>\n",
       "      <td>2023-05-31 23:40:00+04:00</td>\n",
       "      <td>13.669825</td>\n",
       "      <td>0.4756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>2023-05-31 23:45:00+04:00</td>\n",
       "      <td>13.669825</td>\n",
       "      <td>0.4756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8925</th>\n",
       "      <td>2023-05-31 23:50:00+04:00</td>\n",
       "      <td>13.669825</td>\n",
       "      <td>0.4756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8926</th>\n",
       "      <td>2023-05-31 23:55:00+04:00</td>\n",
       "      <td>13.669825</td>\n",
       "      <td>0.4756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8927 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ts  \\\n",
       "0    2023-05-01 00:00:00+04:00   \n",
       "1    2023-05-01 00:05:00+04:00   \n",
       "2    2023-05-01 00:10:00+04:00   \n",
       "3    2023-05-01 00:15:00+04:00   \n",
       "4    2023-05-01 00:20:00+04:00   \n",
       "...                        ...   \n",
       "8922 2023-05-31 23:35:00+04:00   \n",
       "8923 2023-05-31 23:40:00+04:00   \n",
       "8924 2023-05-31 23:45:00+04:00   \n",
       "8925 2023-05-31 23:50:00+04:00   \n",
       "8926 2023-05-31 23:55:00+04:00   \n",
       "\n",
       "      DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy  \\\n",
       "0                                             18.262186                  \n",
       "1                                             20.425119                  \n",
       "2                                             15.942257                  \n",
       "3                                             15.942257                  \n",
       "4                                             15.792557                  \n",
       "...                                                 ...                  \n",
       "8922                                          13.669825                  \n",
       "8923                                          13.669825                  \n",
       "8924                                          13.669825                  \n",
       "8925                                          13.669825                  \n",
       "8926                                          13.669825                  \n",
       "\n",
       "      DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy  \n",
       "0                                                0.0000                              \n",
       "1                                                0.1494                              \n",
       "2                                                0.2525                              \n",
       "3                                                0.2525                              \n",
       "4                                                0.2525                              \n",
       "...                                                 ...                              \n",
       "8922                                             0.4756                              \n",
       "8923                                             0.4756                              \n",
       "8924                                             0.4756                              \n",
       "8925                                             0.4756                              \n",
       "8926                                             0.4756                              \n",
       "\n",
       "[8927 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from data_extraction.skyspark_data_extractor import extractData\n",
    "# from project.models.seasonalNaive import seasonal_naive\n",
    "# from project.models.dynamic_optimized_theta import dynamic_optimized_theta\n",
    "# from models.iterativeImputation import iterative_Imputation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsforecast import StatsForecast\n",
    "\n",
    "import re\n",
    "from statsforecast.models import (\n",
    "    DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "master_table = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "master_table.at[0,\"his\"]\n",
    "# def extractData(data):\n",
    "#     \"\"\"\n",
    "#     Function that extracts data for python from the SS grid.\n",
    "\n",
    "#     Input:\n",
    "#     - data: hisGrid (<class 'hxpy.haystack.grid.Grid>)\n",
    "#     Output:\n",
    "#     - DataFrame with following columns \n",
    "#         - pointID => point id of target variable\n",
    "#         - unit\n",
    "#         - dqType => type of data quality issue\n",
    "#         - dqStart => timestamp of start of data quality issue\n",
    "#         - dqDuration => duration of data quality issue\n",
    "#         - pointInterval => logging interval for the point\n",
    "#         - features => point ids of model features\n",
    "#         - his => history to be used as training data\n",
    "\n",
    "#     ** NOTE_: this function is written to mainly be compatable with python on SS. Running it locally will not work (since it is designed for \n",
    "#     an input of <class 'hxpy.haystack.grid.Grid> type from SS) \n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     # convert the Grid object to df to be able to manipulate it (capitalizing on the hxPy facilitation using the .to_dataframe() function)\n",
    "#     ssData = data.to_dataframe()\n",
    "\n",
    "#     # initiate a new empty dataframe to construct the output\n",
    "#     pythonDF = pd.DataFrame()\n",
    "\n",
    "#     # loop over the ssData and extract the data from each row\n",
    "#     for i in range(len(ssData)):\n",
    "#         pythonDF.loc[i, 'pointID'] = ssData['id'].iloc[i]\n",
    "#         pythonDF.loc[i, 'unit'] = ssData[\"unit\"].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqType'] = ssData[\"dqType\"].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqStart'] = ssData['ts'].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqDuration'] = pd.Timedelta(ssData['dur'].iloc[i], \"min\")\n",
    "#         pythonDF.loc[i, 'pointInterval'] =  pd.Timedelta(ssData[\"freq\"].iloc[i], \"min\" )\n",
    "#         pythonDF.loc[i, 'features'] =  ssData['featId'].iloc[i]\n",
    "#         pythonDF.loc[i, 'his'] =  ssData['data'].iloc[i]#.to_dataframe()\n",
    "        \n",
    "#     return pythonDF\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     Inputs\n",
    "#     df: df used for training set (from SS)\n",
    "#     length_of_missing_data: interval length of missing data (from SS)\n",
    "#     data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "#     Output\n",
    "#     forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "#     # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "#     #df = df.to_dataframe()\n",
    "#     #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "#     # rename the first column as \"target\"\n",
    "#     new_column_name = \"target\"\n",
    "#     df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "#     # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "#     df = df[df.index < dqStart]\n",
    "\n",
    "#     # format the df to statsforecast format\n",
    "#     df = df.reset_index()\n",
    "#     df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "#     df['unique_id'] = \"v0\"    \n",
    "\n",
    "#     # number of predictions\n",
    "#     horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "#     # season length\n",
    "#     season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "#     # frequency\n",
    "#     freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "#     # LIST OF MODELS\n",
    "#     models = [\n",
    "#         DOT(season_length=season_length) \n",
    "#     ]\n",
    "\n",
    "#     # The Model\n",
    "#     sf = StatsForecast( \n",
    "#         models=models,\n",
    "#         freq=freq, \n",
    "#         # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "#         n_jobs=-1,\n",
    "#     )\n",
    "\n",
    "#     # Model fitting\n",
    "#     forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "#     # removing the -hi- and -lo- columns\n",
    "#     for col in forecasts_df.columns:\n",
    "#         if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "#             forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "#     forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "#     forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "#     return forecasts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seasonal_naive(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        SeasonalNaive(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"SeasonalNaive\":\"seasonalNaive\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        DOT(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNeighbors_Regressor_Uniform(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    master_table: main table received from SS\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing columns. Index names as ts\n",
    "    \"\"\"\n",
    "    df = df.at[0,\"his\"]\n",
    "    df = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    df[\"status\"] = df.isna().any(axis=1)\n",
    "    df_predict = df[df[\"status\"]==1]\n",
    "    X_predict = df_predict.iloc[:,1:-1] \n",
    "\n",
    "\n",
    "    # Filtered master table\n",
    "    df_train = df.dropna()\n",
    "    df_train\n",
    "\n",
    "    # Load the dataset\n",
    "    X = df_train.iloc[:,1:-1]  #Enable for SS\n",
    "    y = df_train.iloc[:,0:1]   #Enable for SS\n",
    "\n",
    "    # y = mt_train.iloc[:,1:-1]    #Custom due to sample dataset\n",
    "    # X = mt_train.iloc[:,0:1]     #Custom due to sample dataset\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # # Apply KNN regression\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    predictions = knn_regressor.predict(X_test)\n",
    "    predictions\n",
    "    # Evaluate the model\n",
    "    print('Score:', knn_regressor.score(X_test, y_test))\n",
    "\n",
    "    predict = knn_regressor.predict(X_predict)\n",
    "    df = pd.DataFrame(data=predict, index=X_predict.index, columns=['y_pred'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7396127983489968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:00:00+04:00</th>\n",
       "      <td>18.320856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:05:00+04:00</th>\n",
       "      <td>18.046777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:10:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:15:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:20:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:35:00+04:00</th>\n",
       "      <td>16.498438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>16.498438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y_pred\n",
       "ts                                  \n",
       "2023-05-10 00:00:00+04:00  18.320856\n",
       "2023-05-10 00:05:00+04:00  18.046777\n",
       "2023-05-10 00:10:00+04:00  16.561168\n",
       "2023-05-10 00:15:00+04:00  16.561168\n",
       "2023-05-10 00:20:00+04:00  16.561168\n",
       "...                              ...\n",
       "2023-05-11 23:35:00+04:00  16.498438\n",
       "2023-05-11 23:40:00+04:00  16.498438\n",
       "2023-05-11 23:45:00+04:00  16.042078\n",
       "2023-05-11 23:50:00+04:00  16.042078\n",
       "2023-05-11 23:55:00+04:00  16.042078\n",
       "\n",
       "[576 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_missing_data = 0\n",
    "data_logging_interval = 0\n",
    "dqStart = 0\n",
    "\n",
    "kNeighbors_Regressor_Uniform(master_table,length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "df = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "\n",
    "df = df.at[0,\"his\"]\n",
    "df = df.set_index([\"ts\"])\n",
    "\n",
    "# Tag and filter rows with missing\n",
    "df[\"status\"] = df.isna().any(axis=1)\n",
    "df_predict = df[df[\"status\"]==1]\n",
    "X_predict = df_predict.iloc[:,1:-1] \n",
    "\n",
    "# df_train = df.dropna()\n",
    "# X = df_train.iloc[:,1:-1]\n",
    "# y = df_train.iloc[:,0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy</th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:00:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:05:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:10:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:15:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:20:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:35:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy  \\\n",
       "ts                                                                                            \n",
       "2023-05-10 00:00:00+04:00                                                NaN                  \n",
       "2023-05-10 00:05:00+04:00                                                NaN                  \n",
       "2023-05-10 00:10:00+04:00                                                NaN                  \n",
       "2023-05-10 00:15:00+04:00                                                NaN                  \n",
       "2023-05-10 00:20:00+04:00                                                NaN                  \n",
       "...                                                                      ...                  \n",
       "2023-05-11 23:35:00+04:00                                                NaN                  \n",
       "2023-05-11 23:40:00+04:00                                                NaN                  \n",
       "2023-05-11 23:45:00+04:00                                                NaN                  \n",
       "2023-05-11 23:50:00+04:00                                                NaN                  \n",
       "2023-05-11 23:55:00+04:00                                                NaN                  \n",
       "\n",
       "                           DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy  \\\n",
       "ts                                                                                                         \n",
       "2023-05-10 00:00:00+04:00                                             0.0317                               \n",
       "2023-05-10 00:05:00+04:00                                             0.1968                               \n",
       "2023-05-10 00:10:00+04:00                                             0.2718                               \n",
       "2023-05-10 00:15:00+04:00                                             0.2718                               \n",
       "2023-05-10 00:20:00+04:00                                             0.2718                               \n",
       "...                                                                      ...                               \n",
       "2023-05-11 23:35:00+04:00                                             0.2993                               \n",
       "2023-05-11 23:40:00+04:00                                             0.2993                               \n",
       "2023-05-11 23:45:00+04:00                                             0.2892                               \n",
       "2023-05-11 23:50:00+04:00                                             0.2892                               \n",
       "2023-05-11 23:55:00+04:00                                             0.2892                               \n",
       "\n",
       "                           status  \n",
       "ts                                 \n",
       "2023-05-10 00:00:00+04:00    True  \n",
       "2023-05-10 00:05:00+04:00    True  \n",
       "2023-05-10 00:10:00+04:00    True  \n",
       "2023-05-10 00:15:00+04:00    True  \n",
       "2023-05-10 00:20:00+04:00    True  \n",
       "...                           ...  \n",
       "2023-05-11 23:35:00+04:00    True  \n",
       "2023-05-11 23:40:00+04:00    True  \n",
       "2023-05-11 23:45:00+04:00    True  \n",
       "2023-05-11 23:50:00+04:00    True  \n",
       "2023-05-11 23:55:00+04:00    True  \n",
       "\n",
       "[576 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "\n",
    "df = df.at[0,\"his\"]\n",
    "df = df.set_index([\"ts\"])\n",
    "\n",
    "# Tag and filter rows with missing\n",
    "df[\"status\"] = df.isna().any(axis=1)\n",
    "df_predict = df[df[\"status\"]==1]\n",
    "X_predict = df_predict.iloc[:,1:-1] \n",
    "df_predict\n",
    "#Add the timestmp before and after the gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(python_master_table):\n",
    "    \"\"\"\n",
    "    Function to run all models, and return the one with lowest RMSE.\n",
    "    Models running through the ensemble model will have input DataFrame (AKA the \"his\" column on master_table) \n",
    "    with timestamp as index, target variable as first column, feature variables as the rest of the columns.\n",
    "\n",
    "    Make sure the output predictions of all models are INCLUSIVE of both the \"start ts\" and \"end ts\" (AKA\n",
    "    last ts with real data before gap, and first ts with real data after gap) \n",
    "\n",
    "    Make sure to follow camelCase for DataFrame column naming for compatibility with SS\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary to save predictions for each point\n",
    "    scores_df_dict = {\n",
    "    \"pointID\": [],\n",
    "    \"predictions\": [],\n",
    "    \"rmse\": [],\n",
    "    \"modelName\": []\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "    for i, row in python_master_table.iterrows():\n",
    "\n",
    "        #-----------------\n",
    "        # INPUTS TO MODELS\n",
    "        #-----------------\n",
    "\n",
    "        pointID = row[\"pointID\"]\n",
    "        df = row[\"his\"]#.to_dataframe()                           #### IMPORTANT : UNCOMMENT THIS ON SS\n",
    "        df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "        length_of_missing_data = row[\"dqDuration\"]\n",
    "        data_logging_interval = row[\"pointInterval\"]\n",
    "        dqStart = row[\"dqStart\"]\n",
    "\n",
    "        #----------------------------\n",
    "        # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE \n",
    "        #----------------------------\n",
    "\n",
    "        dq_models = {\n",
    "            \"Seasonal Naive\" : seasonal_naive,\n",
    "            \"Dynamic Optimized Theta\": dynamic_optimized_theta\n",
    "            # \"kNeighbors Regressor Uniform\": kNeighbors_Regressor_Uniform\n",
    "        }\n",
    "\n",
    "        for model_name, model in dq_models.items():\n",
    "            \n",
    "            #------------------------\n",
    "            # ** Calculating RMSE **\n",
    "            #------------------------\n",
    "\n",
    "            # number of predictions needed\n",
    "            horizon = int(length_of_missing_data/data_logging_interval) +1 # why +1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions INCLUSIVE of BOTH start and end ts\n",
    "\n",
    "            # training set size (relative to the horizon/prediction size)\n",
    "            training_set_size = horizon * 10\n",
    "\n",
    "            # training / testing set to evaluate the model (relative to horizon of prediction)\n",
    "            train_data = df.iloc[-1*int(training_set_size):-1*int(horizon)]\n",
    "            test_data = df.iloc[-1*int(horizon):]\n",
    "\n",
    "            # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "            predictions_for_rmse = model(train_data, length_of_missing_data, data_logging_interval, dqStart)\n",
    "            rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "\n",
    "            #------------------\n",
    "            # ** Predictions **\n",
    "            #------------------\n",
    "\n",
    "            # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "            predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "            # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "            start = row['dqStart']\n",
    "            duration = row['dqDuration']\n",
    "            interval = row['pointInterval']\n",
    "            timestamps = pd.date_range(start=start, end=start+duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "            predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "            # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "            predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "            # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "            predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "            # append data to the scores DF\n",
    "            row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                            \"rmse\": rmse_score, \"modelName\": model_name, \n",
    "                            \"identifier\": \n",
    "                                str(row[\"pointID\"])\n",
    "                                +str(row[\"dqStart\"])\n",
    "                                +str(row[\"dqDuration\"])\n",
    "                                +str(row[\"dqType\"])}\n",
    "            \n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "            # return predictions with least RMSE for each point/dq issue\n",
    "            # idx = scores_df.groupby('identifier')['rmse'].idxmin()\n",
    "            # scores_df = scores_df.loc[idx].reset_index(drop=True)\n",
    "            \n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "number sections must be larger than 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mensemble_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 64\u001b[0m, in \u001b[0;36mensemble_model\u001b[1;34m(python_master_table)\u001b[0m\n\u001b[0;32m     61\u001b[0m test_data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mint\u001b[39m(horizon):]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# the prediction. USED ONLY TO EVALUATE RMSE\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m predictions_for_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_of_missing_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_logging_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdqStart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m rmse_score \u001b[38;5;241m=\u001b[39m mean_squared_error(test_data[test_data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy(), predictions_for_rmse[predictions_for_rmse\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_numpy(), squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#------------------\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# ** Predictions **\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m#------------------\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# the predictions. USED FOR DATA CLEANING (uses all the data as training)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m, in \u001b[0;36mseasonal_naive\u001b[1;34m(df, length_of_missing_data, data_logging_interval, dqStart)\u001b[0m\n\u001b[0;32m     45\u001b[0m sf \u001b[38;5;241m=\u001b[39m StatsForecast( \n\u001b[0;32m     46\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m     47\u001b[0m     freq\u001b[38;5;241m=\u001b[39mfreq, \n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# fallback_model = SeasonalNaive(season_length=season_length),\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Model fitting\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m forecasts_df \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munique_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# removing the -hi- and -lo- columns\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m forecasts_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:1622\u001b[0m, in \u001b[0;36mStatsForecast.forecast\u001b[1;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m   1618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must specify `level` when using `prediction_intervals`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1620\u001b[0m     )\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_native(df\u001b[38;5;241m=\u001b[39mdf):\n\u001b[1;32m-> 1622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfitted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprediction_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_intervals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1635\u001b[0m engine \u001b[38;5;241m=\u001b[39m make_execution_engine(infer_by\u001b[38;5;241m=\u001b[39m[df])\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:941\u001b[0m, in \u001b[0;36m_StatsForecast.forecast\u001b[1;34m(self, h, df, X_df, level, fitted, sort_df, prediction_intervals, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m    930\u001b[0m     res_fcsts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mga\u001b[38;5;241m.\u001b[39mforecast(\n\u001b[0;32m    931\u001b[0m         models\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels,\n\u001b[0;32m    932\u001b[0m         h\u001b[38;5;241m=\u001b[39mh,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m         target_col\u001b[38;5;241m=\u001b[39mtarget_col,\n\u001b[0;32m    939\u001b[0m     )\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     res_fcsts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forecast_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfitted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fitted:\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfcst_fitted_values_ \u001b[38;5;241m=\u001b[39m res_fcsts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitted\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:1245\u001b[0m, in \u001b[0;36m_StatsForecast._forecast_parallel\u001b[1;34m(self, h, fitted, X, level, target_col)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forecast_parallel\u001b[39m(\u001b[38;5;28mself\u001b[39m, h, fitted, X, level, target_col):\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# create elements for each core\u001b[39;00m\n\u001b[1;32m-> 1245\u001b[0m     gas, Xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gas_Xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1246\u001b[0m     Pool, pool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pool()\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# compute parallel forecasts\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:1194\u001b[0m, in \u001b[0;36m_StatsForecast._get_gas_Xs\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_gas_Xs\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1194\u001b[0m     gas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mga\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1196\u001b[0m         Xs \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\statsforecast\\core.py:383\u001b[0m, in \u001b[0;36mGroupedArray.split\u001b[1;34m(self, n_chunks)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_chunks):\n\u001b[0;32m    381\u001b[0m     n_chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_chunks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_groups)\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 383\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(idxs) \u001b[38;5;28;01mfor\u001b[39;00m idxs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_groups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ccarandang\\AppData\\Local\\anaconda3\\envs\\ssv1\\Lib\\site-packages\\numpy\\lib\\shape_base.py:770\u001b[0m, in \u001b[0;36marray_split\u001b[1;34m(ary, indices_or_sections, axis)\u001b[0m\n\u001b[0;32m    768\u001b[0m Nsections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(indices_or_sections)\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Nsections \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber sections must be larger than 0.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    771\u001b[0m Neach_section, extras \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(Ntotal, Nsections)\n\u001b[0;32m    772\u001b[0m section_sizes \u001b[38;5;241m=\u001b[39m ([\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    773\u001b[0m                  extras \u001b[38;5;241m*\u001b[39m [Neach_section\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m    774\u001b[0m                  (Nsections\u001b[38;5;241m-\u001b[39mextras) \u001b[38;5;241m*\u001b[39m [Neach_section])\n",
      "\u001b[1;31mValueError\u001b[0m: number sections must be larger than 0."
     ]
    }
   ],
   "source": [
    "ensemble_model(master_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
