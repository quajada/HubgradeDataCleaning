{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>val</th>\n",
       "      <th>discharge_Chilled_Water_Cool_Valve_Cmd_AHU_01_B1</th>\n",
       "      <th>discharge_Fan_Vfd_Speed_Sensor_AHU_01_B1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-02 14:05:00+04:00</td>\n",
       "      <td>23.808224</td>\n",
       "      <td>41.660519</td>\n",
       "      <td>61.804546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-02 14:10:00+04:00</td>\n",
       "      <td>23.808224</td>\n",
       "      <td>41.660519</td>\n",
       "      <td>61.669575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-02 14:15:00+04:00</td>\n",
       "      <td>23.808224</td>\n",
       "      <td>41.660519</td>\n",
       "      <td>61.407654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-02 14:20:00+04:00</td>\n",
       "      <td>23.808224</td>\n",
       "      <td>41.660519</td>\n",
       "      <td>61.641323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-02 14:25:00+04:00</td>\n",
       "      <td>23.808224</td>\n",
       "      <td>41.660519</td>\n",
       "      <td>61.659443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>2023-03-19 00:45:00+04:00</td>\n",
       "      <td>22.633480</td>\n",
       "      <td>29.431870</td>\n",
       "      <td>57.147915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>2023-03-19 00:50:00+04:00</td>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.548756</td>\n",
       "      <td>57.144928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>2023-03-19 00:55:00+04:00</td>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "      <td>57.145638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>2023-03-19 01:00:00+04:00</td>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "      <td>57.148205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>2023-03-19 01:05:00+04:00</td>\n",
       "      <td>22.633480</td>\n",
       "      <td>30.514156</td>\n",
       "      <td>57.146976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4741 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Timestamp        val  \\\n",
       "0    2023-03-02 14:05:00+04:00  23.808224   \n",
       "1    2023-03-02 14:10:00+04:00  23.808224   \n",
       "2    2023-03-02 14:15:00+04:00  23.808224   \n",
       "3    2023-03-02 14:20:00+04:00  23.808224   \n",
       "4    2023-03-02 14:25:00+04:00  23.808224   \n",
       "...                        ...        ...   \n",
       "4736 2023-03-19 00:45:00+04:00  22.633480   \n",
       "4737 2023-03-19 00:50:00+04:00  22.633480   \n",
       "4738 2023-03-19 00:55:00+04:00  22.633480   \n",
       "4739 2023-03-19 01:00:00+04:00  22.633480   \n",
       "4740 2023-03-19 01:05:00+04:00  22.633480   \n",
       "\n",
       "      discharge_Chilled_Water_Cool_Valve_Cmd_AHU_01_B1  \\\n",
       "0                                            41.660519   \n",
       "1                                            41.660519   \n",
       "2                                            41.660519   \n",
       "3                                            41.660519   \n",
       "4                                            41.660519   \n",
       "...                                                ...   \n",
       "4736                                         29.431870   \n",
       "4737                                         30.548756   \n",
       "4738                                         30.514156   \n",
       "4739                                         30.514156   \n",
       "4740                                         30.514156   \n",
       "\n",
       "      discharge_Fan_Vfd_Speed_Sensor_AHU_01_B1  \n",
       "0                                    61.804546  \n",
       "1                                    61.669575  \n",
       "2                                    61.407654  \n",
       "3                                    61.641323  \n",
       "4                                    61.659443  \n",
       "...                                        ...  \n",
       "4736                                 57.147915  \n",
       "4737                                 57.144928  \n",
       "4738                                 57.145638  \n",
       "4739                                 57.148205  \n",
       "4740                                 57.146976  \n",
       "\n",
       "[4741 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from data_extraction.skyspark_data_extractor import extractData\n",
    "# from project.models.seasonalNaive import seasonal_naive\n",
    "# from project.models.dynamic_optimized_theta import dynamic_optimized_theta\n",
    "# from models.iterativeImputation import iterative_Imputation\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsforecast import StatsForecast\n",
    "\n",
    "import re\n",
    "from statsforecast.models import (\n",
    "    DynamicOptimizedTheta as DOT,\n",
    "    SeasonalNaive,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "master_table = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "master_table.at[1,\"his\"]\n",
    "# def extractData(data):\n",
    "#     \"\"\"\n",
    "#     Function that extracts data for python from the SS grid.\n",
    "\n",
    "#     Input:\n",
    "#     - data: hisGrid (<class 'hxpy.haystack.grid.Grid>)\n",
    "#     Output:\n",
    "#     - DataFrame with following columns \n",
    "#         - pointID => point id of target variable\n",
    "#         - unit\n",
    "#         - dqType => type of data quality issue\n",
    "#         - dqStart => timestamp of start of data quality issue\n",
    "#         - dqDuration => duration of data quality issue\n",
    "#         - pointInterval => logging interval for the point\n",
    "#         - features => point ids of model features\n",
    "#         - his => history to be used as training data\n",
    "\n",
    "#     ** NOTE_: this function is written to mainly be compatable with python on SS. Running it locally will not work (since it is designed for \n",
    "#     an input of <class 'hxpy.haystack.grid.Grid> type from SS) \n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     # convert the Grid object to df to be able to manipulate it (capitalizing on the hxPy facilitation using the .to_dataframe() function)\n",
    "#     ssData = data.to_dataframe()\n",
    "\n",
    "#     # initiate a new empty dataframe to construct the output\n",
    "#     pythonDF = pd.DataFrame()\n",
    "\n",
    "#     # loop over the ssData and extract the data from each row\n",
    "#     for i in range(len(ssData)):\n",
    "#         pythonDF.loc[i, 'pointID'] = ssData['id'].iloc[i]\n",
    "#         pythonDF.loc[i, 'unit'] = ssData[\"unit\"].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqType'] = ssData[\"dqType\"].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqStart'] = ssData['ts'].iloc[i]\n",
    "#         pythonDF.loc[i, 'dqDuration'] = pd.Timedelta(ssData['dur'].iloc[i], \"min\")\n",
    "#         pythonDF.loc[i, 'pointInterval'] =  pd.Timedelta(ssData[\"freq\"].iloc[i], \"min\" )\n",
    "#         pythonDF.loc[i, 'features'] =  ssData['featId'].iloc[i]\n",
    "#         pythonDF.loc[i, 'his'] =  ssData['data'].iloc[i]#.to_dataframe()\n",
    "        \n",
    "#     return pythonDF\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     Inputs\n",
    "#     df: df used for training set (from SS)\n",
    "#     length_of_missing_data: interval length of missing data (from SS)\n",
    "#     data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "#     Output\n",
    "#     forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "#     # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "#     #df = df.to_dataframe()\n",
    "#     #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "#     # rename the first column as \"target\"\n",
    "#     new_column_name = \"target\"\n",
    "#     df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "#     # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "#     df = df[df.index < dqStart]\n",
    "\n",
    "#     # format the df to statsforecast format\n",
    "#     df = df.reset_index()\n",
    "#     df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "#     df['unique_id'] = \"v0\"    \n",
    "\n",
    "#     # number of predictions\n",
    "#     horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "#     # season length\n",
    "#     season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "#     # frequency\n",
    "#     freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "#     # LIST OF MODELS\n",
    "#     models = [\n",
    "#         DOT(season_length=season_length) \n",
    "#     ]\n",
    "\n",
    "#     # The Model\n",
    "#     sf = StatsForecast( \n",
    "#         models=models,\n",
    "#         freq=freq, \n",
    "#         # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "#         n_jobs=-1,\n",
    "#     )\n",
    "\n",
    "#     # Model fitting\n",
    "#     forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "#     # removing the -hi- and -lo- columns\n",
    "#     for col in forecasts_df.columns:\n",
    "#         if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "#             forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "#     forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "#     forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "#     return forecasts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seasonal_naive(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        SeasonalNaive(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"SeasonalNaive\":\"seasonalNaive\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_optimized_theta(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Inputs\n",
    "    df: df used for training set (from SS)\n",
    "    length_of_missing_data: interval length of missing data (from SS)\n",
    "    data_logging_interval: data logging interval - called from the hisDQInterval tag on the point (from SS)\n",
    "\n",
    "    Output\n",
    "    forecasts_df: dataframe with predictions for the period missing data. Index names as ts, values column named as \"v0\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # step 1 convert the grid to a dataframe, and set first column as index     ### UNCOMMENT THIS ONLY IF RUNNING THE MODEL DIRECTLY ON SS. THIS IS DONE IN THE ENSEMBLE MODEL SO NO NEED TO HAVE THIS WHEN RUNNING THROUGH ENSEMBLE MODEL\n",
    "    #df = df.to_dataframe()\n",
    "    #df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "\n",
    "    # rename the first column as \"target\"\n",
    "    new_column_name = \"target\"\n",
    "    df = df.rename(columns={df.columns[0]: new_column_name})\n",
    "\n",
    "    # keep only the history BEFORE the start of the data quality issue, since this is a statisitcal model not ML model\n",
    "    df = df[df.index < dqStart]\n",
    "\n",
    "    # format the df to statsforecast format\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={df.columns[0]: 'ds', df.columns[1]: \"y\"})\n",
    "    df['unique_id'] = \"v0\"    \n",
    "\n",
    "    # number of predictions\n",
    "    horizon = int(length_of_missing_data/data_logging_interval) + 1 # why -1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions also for the start and end timestamp. Can remove them later\n",
    "\n",
    "    # season length\n",
    "    season_length = int(pd.Timedelta(24, 'h') / data_logging_interval)      \n",
    "\n",
    "    # frequency\n",
    "    freq = str(data_logging_interval.total_seconds()/3600)+\"h\"\n",
    "\n",
    "\n",
    "    # LIST OF MODELS\n",
    "    models = [\n",
    "        DOT(season_length=season_length) \n",
    "    ]\n",
    "\n",
    "    # The Model\n",
    "    sf = StatsForecast( \n",
    "        models=models,\n",
    "        freq=freq, \n",
    "        # fallback_model = SeasonalNaive(season_length=season_length),\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # Model fitting\n",
    "    forecasts_df = sf.forecast(df=df[[\"ds\", \"y\", \"unique_id\"]], h=horizon, level=[90])  \n",
    "\n",
    "    # removing the -hi- and -lo- columns\n",
    "    for col in forecasts_df.columns:\n",
    "        if re.search(\"-hi-\", col) or re.search(\"-lo-\", col):\n",
    "            forecasts_df.drop(col, axis=1, inplace=True)\n",
    "            \n",
    "    forecasts_df = forecasts_df.rename(columns={\"ds\": \"timestamp\", \"DynamicOptimizedTheta\":\"dynamicOptimizedTheta\"})\n",
    "\n",
    "    forecasts_df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    return forecasts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNeighbors_Regressor_Uniform(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    master_table: main table received from SS\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing columns. Index names as ts\n",
    "    \"\"\"\n",
    "    df = df.at[0,\"his\"]\n",
    "    df = df.set_index([\"ts\"])\n",
    "\n",
    "    # Tag and filter rows with missing\n",
    "    df[\"status\"] = df.isna().any(axis=1)\n",
    "    df_predict = df[df[\"status\"]==1]\n",
    "    X_predict = df_predict.iloc[:,1:-1] \n",
    "\n",
    "\n",
    "    # Filtered master table\n",
    "    df_train = df.dropna()\n",
    "    df_train\n",
    "\n",
    "    # Load the dataset\n",
    "    X = df_train.iloc[:,1:-1]  #Enable for SS\n",
    "    y = df_train.iloc[:,0:1]   #Enable for SS\n",
    "\n",
    "    # y = mt_train.iloc[:,1:-1]    #Custom due to sample dataset\n",
    "    # X = mt_train.iloc[:,0:1]     #Custom due to sample dataset\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # # Apply KNN regression\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    predictions = knn_regressor.predict(X_test)\n",
    "    predictions\n",
    "    # Evaluate the model\n",
    "    print('Score:', knn_regressor.score(X_test, y_test))\n",
    "\n",
    "    predict = knn_regressor.predict(X_predict)\n",
    "    df = pd.DataFrame(data=predict, index=X_predict.index, columns=['y_pred'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7396127983489968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:00:00+04:00</th>\n",
       "      <td>18.320856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:05:00+04:00</th>\n",
       "      <td>18.046777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:10:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:15:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:20:00+04:00</th>\n",
       "      <td>16.561168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:35:00+04:00</th>\n",
       "      <td>16.498438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>16.498438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>16.042078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y_pred\n",
       "ts                                  \n",
       "2023-05-10 00:00:00+04:00  18.320856\n",
       "2023-05-10 00:05:00+04:00  18.046777\n",
       "2023-05-10 00:10:00+04:00  16.561168\n",
       "2023-05-10 00:15:00+04:00  16.561168\n",
       "2023-05-10 00:20:00+04:00  16.561168\n",
       "...                              ...\n",
       "2023-05-11 23:35:00+04:00  16.498438\n",
       "2023-05-11 23:40:00+04:00  16.498438\n",
       "2023-05-11 23:45:00+04:00  16.042078\n",
       "2023-05-11 23:50:00+04:00  16.042078\n",
       "2023-05-11 23:55:00+04:00  16.042078\n",
       "\n",
       "[576 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_of_missing_data = 0\n",
    "data_logging_interval = 0\n",
    "dqStart = 0\n",
    "\n",
    "kNeighbors_Regressor_Uniform(master_table,length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "df = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "\n",
    "df = df.at[0,\"his\"]\n",
    "df = df.set_index([\"ts\"])\n",
    "\n",
    "# Tag and filter rows with missing\n",
    "df[\"status\"] = df.isna().any(axis=1)\n",
    "df_predict = df[df[\"status\"]==1]\n",
    "X_predict = df_predict.iloc[:,1:-1] \n",
    "\n",
    "# df_train = df.dropna()\n",
    "# X = df_train.iloc[:,1:-1]\n",
    "# y = df_train.iloc[:,0:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy</th>\n",
       "      <th>DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:00:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:05:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1968</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:10:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:15:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:20:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2718</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:35:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2993</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>576 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           DMC Building 1 Data Quality Tests Discharge-Air-Temp-Sensor Copy  \\\n",
       "ts                                                                                            \n",
       "2023-05-10 00:00:00+04:00                                                NaN                  \n",
       "2023-05-10 00:05:00+04:00                                                NaN                  \n",
       "2023-05-10 00:10:00+04:00                                                NaN                  \n",
       "2023-05-10 00:15:00+04:00                                                NaN                  \n",
       "2023-05-10 00:20:00+04:00                                                NaN                  \n",
       "...                                                                      ...                  \n",
       "2023-05-11 23:35:00+04:00                                                NaN                  \n",
       "2023-05-11 23:40:00+04:00                                                NaN                  \n",
       "2023-05-11 23:45:00+04:00                                                NaN                  \n",
       "2023-05-11 23:50:00+04:00                                                NaN                  \n",
       "2023-05-11 23:55:00+04:00                                                NaN                  \n",
       "\n",
       "                           DMC Building 1 Data Quality Tests Discharge-Chilled-Water-Cool-Valve-Cmd Copy  \\\n",
       "ts                                                                                                         \n",
       "2023-05-10 00:00:00+04:00                                             0.0317                               \n",
       "2023-05-10 00:05:00+04:00                                             0.1968                               \n",
       "2023-05-10 00:10:00+04:00                                             0.2718                               \n",
       "2023-05-10 00:15:00+04:00                                             0.2718                               \n",
       "2023-05-10 00:20:00+04:00                                             0.2718                               \n",
       "...                                                                      ...                               \n",
       "2023-05-11 23:35:00+04:00                                             0.2993                               \n",
       "2023-05-11 23:40:00+04:00                                             0.2993                               \n",
       "2023-05-11 23:45:00+04:00                                             0.2892                               \n",
       "2023-05-11 23:50:00+04:00                                             0.2892                               \n",
       "2023-05-11 23:55:00+04:00                                             0.2892                               \n",
       "\n",
       "                           status  \n",
       "ts                                 \n",
       "2023-05-10 00:00:00+04:00    True  \n",
       "2023-05-10 00:05:00+04:00    True  \n",
       "2023-05-10 00:10:00+04:00    True  \n",
       "2023-05-10 00:15:00+04:00    True  \n",
       "2023-05-10 00:20:00+04:00    True  \n",
       "...                           ...  \n",
       "2023-05-11 23:35:00+04:00    True  \n",
       "2023-05-11 23:40:00+04:00    True  \n",
       "2023-05-11 23:45:00+04:00    True  \n",
       "2023-05-11 23:50:00+04:00    True  \n",
       "2023-05-11 23:55:00+04:00    True  \n",
       "\n",
       "[576 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "\n",
    "df = df.at[0,\"his\"]\n",
    "df = df.set_index([\"ts\"])\n",
    "\n",
    "# Tag and filter rows with missing\n",
    "df[\"status\"] = df.isna().any(axis=1)\n",
    "df_predict = df[df[\"status\"]==1]\n",
    "X_predict = df_predict.iloc[:,1:-1] \n",
    "df_predict\n",
    "#Add the timestmp before and after the gap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_model(python_master_table):\n",
    "    \"\"\"\n",
    "    Function to run all models, and return the one with lowest RMSE.\n",
    "    Models running through the ensemble model will have input DataFrame (AKA the \"his\" column on master_table) \n",
    "    with timestamp as index, target variable as first column, feature variables as the rest of the columns.\n",
    "\n",
    "    Make sure the output predictions of all models are INCLUSIVE of both the \"start ts\" and \"end ts\" (AKA\n",
    "    last ts with real data before gap, and first ts with real data after gap) \n",
    "\n",
    "    Make sure to follow camelCase for DataFrame column naming for compatibility with SS\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary to save predictions for each point\n",
    "    scores_df_dict = {\n",
    "    \"pointID\": [],\n",
    "    \"predictions\": [],\n",
    "    \"rmse\": [],\n",
    "    \"modelName\": []\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    scores_df = pd.DataFrame(scores_df_dict)\n",
    "\n",
    "    for i, row in python_master_table.iterrows():\n",
    "\n",
    "        #-----------------\n",
    "        # INPUTS TO MODELS\n",
    "        #-----------------\n",
    "\n",
    "        pointID = row[\"pointID\"]\n",
    "        df = row[\"his\"]#.to_dataframe()                           #### IMPORTANT : UNCOMMENT THIS ON SS\n",
    "        df.set_index(df.columns[0], inplace=True, drop=True)\n",
    "        length_of_missing_data = row[\"dqDuration\"]\n",
    "        data_logging_interval = row[\"pointInterval\"]\n",
    "        dqStart = row[\"dqStart\"]\n",
    "\n",
    "        #----------------------------\n",
    "        # Dict of Data Quality Models                              ############# ADD NEW MODELS HERE \n",
    "        #----------------------------\n",
    "\n",
    "        dq_models = {\n",
    "            \"Seasonal Naive\" : seasonal_naive,\n",
    "            \"Dynamic Optimized Theta\": dynamic_optimized_theta\n",
    "            # \"kNeighbors Regressor Uniform\": kNeighbors_Regressor_Uniform\n",
    "        }\n",
    "\n",
    "        for model_name, model in dq_models.items():\n",
    "            \n",
    "            #------------------------\n",
    "            # ** Calculating RMSE **\n",
    "            #------------------------\n",
    "\n",
    "            # number of predictions needed\n",
    "            horizon = int(length_of_missing_data/data_logging_interval) +1 # why +1? because if you do length_of_missing_data/data_logging_interval you will get prediction length that is exclusive of the start ts (start ts is the last ts with actual data before the gap), and inclusive of the end ts (end ts is the first ts with actual data after the gap). +1 to get predictions INCLUSIVE of BOTH start and end ts\n",
    "\n",
    "            # training set size (relative to the horizon/prediction size)\n",
    "            training_set_size = horizon * 10\n",
    "\n",
    "            # training / testing set to evaluate the model (relative to horizon of prediction)\n",
    "            train_data = df.iloc[-1*int(training_set_size):-1*int(horizon)]\n",
    "            test_data = df.iloc[-1*int(horizon):]\n",
    "\n",
    "            # the prediction. USED ONLY TO EVALUATE RMSE\n",
    "            predictions_for_rmse = model(train_data, length_of_missing_data, data_logging_interval, dqStart)\n",
    "            rmse_score = mean_squared_error(test_data[test_data.columns[0]].to_numpy(), predictions_for_rmse[predictions_for_rmse.columns[0]].to_numpy(), squared=False)\n",
    "\n",
    "            #------------------\n",
    "            # ** Predictions **\n",
    "            #------------------\n",
    "\n",
    "            # the predictions. USED FOR DATA CLEANING (uses all the data as training)\n",
    "            predictions_for_data_quality = model(df, length_of_missing_data, data_logging_interval, dqStart)\n",
    "\n",
    "            # keep only timestamps for null periods (rows where there are null values on SS)\n",
    "            start = row['dqStart']\n",
    "            duration = row['dqDuration']\n",
    "            interval = row['pointInterval']\n",
    "            timestamps = pd.date_range(start=start, end=start+duration, freq=interval)[1:-1] # clipping the first and last timestamps, as they already exist with actual data on SS\n",
    "\n",
    "            predictions_for_data_quality = predictions_for_data_quality[predictions_for_data_quality.index.isin(timestamps)]\n",
    "\n",
    "            # reset index to make the ts a column instead of index. SS doesnt show the index of a DF\n",
    "            predictions_for_data_quality = predictions_for_data_quality.reset_index()\n",
    "\n",
    "            # rename the ts and predictions column to \"ts\" and \"predictions\", to have similar naming for all ouutputs of models (makes it easier as well when using the dcInsert function on SS.)\n",
    "            predictions_for_data_quality.columns = [\"ts\", \"predictions\"]\n",
    "\n",
    "            # append data to the scores DF\n",
    "            row_to_append = {'pointID': pointID, 'predictions': predictions_for_data_quality, \n",
    "                            \"rmse\": rmse_score, \"modelName\": model_name, \n",
    "                            \"identifier\": \n",
    "                                str(row[\"pointID\"])\n",
    "                                +str(row[\"dqStart\"])\n",
    "                                +str(row[\"dqDuration\"])\n",
    "                                +str(row[\"dqType\"])}\n",
    "            \n",
    "            scores_df = pd.concat([scores_df, pd.DataFrame([row_to_append])], ignore_index=True)\n",
    "\n",
    "            # return predictions with least RMSE for each point/dq issue\n",
    "            # idx = scores_df.groupby('identifier')['rmse'].idxmin()\n",
    "            # scores_df = scores_df.loc[idx].reset_index(drop=True)\n",
    "            \n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-09 23:55:00+04:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "      <th>discharge_Chilled_Water_Cool_Valve_Cmd_AHU_01_B1</th>\n",
       "      <th>discharge_Fan_Vfd_Speed_Sensor_AHU_01_B1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:10:00+04:00</th>\n",
       "      <td>15.832650</td>\n",
       "      <td>24.86</td>\n",
       "      <td>57.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:15:00+04:00</th>\n",
       "      <td>15.981872</td>\n",
       "      <td>28.63</td>\n",
       "      <td>57.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:20:00+04:00</th>\n",
       "      <td>15.981872</td>\n",
       "      <td>32.38</td>\n",
       "      <td>57.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:25:00+04:00</th>\n",
       "      <td>15.981872</td>\n",
       "      <td>33.67</td>\n",
       "      <td>57.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:30:00+04:00</th>\n",
       "      <td>15.981872</td>\n",
       "      <td>35.08</td>\n",
       "      <td>57.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.93</td>\n",
       "      <td>47.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.92</td>\n",
       "      <td>47.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.92</td>\n",
       "      <td>47.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.92</td>\n",
       "      <td>47.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-12 00:00:00+04:00</th>\n",
       "      <td>18.119871</td>\n",
       "      <td>3.28</td>\n",
       "      <td>47.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19521 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 val  \\\n",
       "ts                                     \n",
       "2023-03-01 00:10:00+04:00  15.832650   \n",
       "2023-03-01 00:15:00+04:00  15.981872   \n",
       "2023-03-01 00:20:00+04:00  15.981872   \n",
       "2023-03-01 00:25:00+04:00  15.981872   \n",
       "2023-03-01 00:30:00+04:00  15.981872   \n",
       "...                              ...   \n",
       "2023-05-11 23:40:00+04:00        NaN   \n",
       "2023-05-11 23:45:00+04:00        NaN   \n",
       "2023-05-11 23:50:00+04:00        NaN   \n",
       "2023-05-11 23:55:00+04:00        NaN   \n",
       "2023-05-12 00:00:00+04:00  18.119871   \n",
       "\n",
       "                           discharge_Chilled_Water_Cool_Valve_Cmd_AHU_01_B1  \\\n",
       "ts                                                                            \n",
       "2023-03-01 00:10:00+04:00                                             24.86   \n",
       "2023-03-01 00:15:00+04:00                                             28.63   \n",
       "2023-03-01 00:20:00+04:00                                             32.38   \n",
       "2023-03-01 00:25:00+04:00                                             33.67   \n",
       "2023-03-01 00:30:00+04:00                                             35.08   \n",
       "...                                                                     ...   \n",
       "2023-05-11 23:40:00+04:00                                             29.93   \n",
       "2023-05-11 23:45:00+04:00                                             28.92   \n",
       "2023-05-11 23:50:00+04:00                                             28.92   \n",
       "2023-05-11 23:55:00+04:00                                             28.92   \n",
       "2023-05-12 00:00:00+04:00                                              3.28   \n",
       "\n",
       "                           discharge_Fan_Vfd_Speed_Sensor_AHU_01_B1  \n",
       "ts                                                                   \n",
       "2023-03-01 00:10:00+04:00                                     57.37  \n",
       "2023-03-01 00:15:00+04:00                                     57.41  \n",
       "2023-03-01 00:20:00+04:00                                     57.41  \n",
       "2023-03-01 00:25:00+04:00                                     57.37  \n",
       "2023-03-01 00:30:00+04:00                                     57.31  \n",
       "...                                                             ...  \n",
       "2023-05-11 23:40:00+04:00                                     47.34  \n",
       "2023-05-11 23:45:00+04:00                                     47.17  \n",
       "2023-05-11 23:50:00+04:00                                     47.28  \n",
       "2023-05-11 23:55:00+04:00                                     47.38  \n",
       "2023-05-12 00:00:00+04:00                                     47.41  \n",
       "\n",
       "[19521 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from data_extraction.skyspark_data_extractor import extractData\n",
    "master_table = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "length_of_missing_data = 0\n",
    "data_logging_interval = 0\n",
    "dqStart = \"2023-05-09 23:55:00+04:00\"\n",
    "date_format = '%Y-%m-%d %H:%M:%S%z'\n",
    "date_obj = datetime.strptime(dqStart, date_format)\n",
    "print(date_obj)\n",
    "master_table = master_table.at[1,\"his\"]\n",
    "master_table = master_table.set_index([\"ts\"])\n",
    "master_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ts</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-05-09 23:55:00+04:00</th>\n",
       "      <td>16.114820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:00:00+04:00</th>\n",
       "      <td>18.213834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:05:00+04:00</th>\n",
       "      <td>18.315656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:10:00+04:00</th>\n",
       "      <td>15.889761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-10 00:15:00+04:00</th>\n",
       "      <td>15.839511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:40:00+04:00</th>\n",
       "      <td>16.049794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:45:00+04:00</th>\n",
       "      <td>15.606749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:50:00+04:00</th>\n",
       "      <td>15.744923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-11 23:55:00+04:00</th>\n",
       "      <td>15.641476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-12 00:00:00+04:00</th>\n",
       "      <td>18.225456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              y_pred\n",
       "ts                                  \n",
       "2023-05-09 23:55:00+04:00  16.114820\n",
       "2023-05-10 00:00:00+04:00  18.213834\n",
       "2023-05-10 00:05:00+04:00  18.315656\n",
       "2023-05-10 00:10:00+04:00  15.889761\n",
       "2023-05-10 00:15:00+04:00  15.839511\n",
       "...                              ...\n",
       "2023-05-11 23:40:00+04:00  16.049794\n",
       "2023-05-11 23:45:00+04:00  15.606749\n",
       "2023-05-11 23:50:00+04:00  15.744923\n",
       "2023-05-11 23:55:00+04:00  15.641476\n",
       "2023-05-12 00:00:00+04:00  18.225456\n",
       "\n",
       "[578 rows x 1 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from data_extraction.dummy_data_extractor import extract_dummy_data\n",
    "from data_extraction.skyspark_data_extractor import extractData\n",
    "\n",
    "master_table = extract_dummy_data(r\"C:\\Users\\ccarandang\\OneDrive - Enova Facilities Management\\Documents\\GitHub\\HubgradeDataCleaning\\HubgradeDataCleaning\\christine\\dummy_data\")\n",
    "\n",
    "length_of_missing_data = 0\n",
    "data_logging_interval = 0\n",
    "dqStart = \"2023-05-09 23:55:00+04:00\"\n",
    "date_format = '%Y-%m-%d %H:%M:%S%z'\n",
    "date_obj = datetime.strptime(dqStart, date_format)\n",
    "master_table = master_table.at[1,\"his\"]\n",
    "master_table = master_table.set_index([\"ts\"])\n",
    "df = master_table\n",
    "\n",
    "# Tag and filter rows with missing for checking purposes only\n",
    "df[\"status\"] = df.isna().any(axis=1)\n",
    "df_predict = df[df[\"status\"]==1]\n",
    "df_predict\n",
    "\n",
    "\n",
    "df_test = df[df.index >= dqStart]\n",
    "df_train = df[df.index <= dqStart]\n",
    "\n",
    "\n",
    "X = df.iloc[:,1:-1]\n",
    "y = df.iloc[:,0:1]  \n",
    "\n",
    "X_train = X[X.index <= dqStart]\n",
    "X_test = X[X.index >= dqStart]\n",
    "y_train = y[y.index <= dqStart]\n",
    "y_test =  y[y.index >= dqStart]\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "pred = knn_regressor.predict(X_test)\n",
    "predictions = pd.DataFrame(data=pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def kNeighbors_Regressor_Uniform(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    df:data table from SS with the ts as index\n",
    "    dqStart: start datetime\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing data inclusive of the start date and end date. Index names as ts\n",
    "    \"\"\"\n",
    "    X = df.iloc[:,1:-1]\n",
    "    y = df.iloc[:,0:1]  \n",
    "\n",
    "    X_train = X[X.index <= dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    y_train = y[y.index <= dqStart]\n",
    "\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=3,weights=\"distance\")\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    pred = knn_regressor.predict(X_test)\n",
    "    predictions = pd.DataFrame(data=pred, index=X_test.index, columns=['y_pred'])\n",
    "\n",
    "    predictions\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "def random_Forest_Regressor(df, length_of_missing_data, data_logging_interval, dqStart):\n",
    "    \"\"\"\n",
    "    Input\n",
    "    master_table: main table received from SS\n",
    "\n",
    "    Output\n",
    "    df: dataframe with predictions for all rows with missing columns. Index names as ts\n",
    "    \"\"\"\n",
    "    X = df.iloc[:,1:-1]\n",
    "    y = df.iloc[:,0:1]  \n",
    "\n",
    "    X_train = X[X.index <= dqStart]\n",
    "    X_test = X[X.index >= dqStart]\n",
    "    y_train = y[y.index <= dqStart]\n",
    "\n",
    "    # Fitting Random Forest Regression to the dataset\n",
    "    regressor = RandomForestRegressor(n_estimators=10, random_state=0, oob_score=True)\n",
    "    \n",
    "    # Fit the regressor with x and y data\n",
    "    regressor.fit(X_train, y_train)\n",
    "    pred = regressor.predict(X_test)\n",
    "    predictions = pd.DataFrame(data=pred, index=X_test.index, columns=['y_pred'])\n",
    "    \n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
